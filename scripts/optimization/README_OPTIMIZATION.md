# Optunaè¶…åƒæ•¸å„ªåŒ–ç³»çµ±ä½¿ç”¨æŒ‡å—

## ğŸ“‹ ç³»çµ±æ¦‚è¿°

é€™æ˜¯ä¸€å€‹åŸºæ–¼ Optuna çš„è‡ªå‹•åŒ–è¶…åƒæ•¸å„ªåŒ–ç³»çµ±ï¼Œå°ˆé–€ç‚º Pixar é¢¨æ ¼ 3D å‹•ç•« LoRA è¨“ç·´è¨­è¨ˆã€‚

### æ ¸å¿ƒåŠŸèƒ½

1. **è‡ªå‹•è¶…åƒæ•¸æœç´¢** - ä½¿ç”¨ TPE (Tree-structured Parzen Estimator) ç®—æ³•
2. **å¤šç›®æ¨™å„ªåŒ–** - åŒæ™‚å„ªåŒ– brightness å’Œ contrast
3. **å¢å¼·è©•ä¼°æŒ‡æ¨™** - LPIPSã€CLIP consistencyã€Pixar style score
4. **çµæœåˆ†æ** - è‡ªå‹•ç”Ÿæˆå ±å‘Šã€è¦–è¦ºåŒ–åœ–è¡¨
5. **æœ€ä½³é…ç½®å°å‡º** - è‡ªå‹•ç”Ÿæˆè¨“ç·´é…ç½®æ–‡ä»¶

## ğŸ¯ å„ªåŒ–ç›®æ¨™

é‡å° Pixar é¢¨æ ¼çš„ç‰¹å®šç›®æ¨™ï¼š

- **Brightnessï¼ˆäº®åº¦ï¼‰**: ç›®æ¨™ 0.50ï¼Œç¯„åœ 0.4-0.6
- **Contrastï¼ˆå°æ¯”åº¦ï¼‰**: ç›®æ¨™ 0.20ï¼Œç¯„åœ 0.15-0.25ï¼ˆPixar ä½å°æ¯”ç‰¹å¾µï¼‰
- **Consistencyï¼ˆä¸€è‡´æ€§ï¼‰**: é™ä½ brightness å’Œ contrast çš„æ¨™æº–å·®

## ğŸ”§ è¶…åƒæ•¸æœç´¢ç©ºé–“

### å­¸ç¿’ç‡ï¼ˆæœ€é—œéµï¼‰
- `learning_rate`: 5e-5 åˆ° 2e-4ï¼ˆå°æ•¸å‡å‹»åˆ†å¸ƒï¼‰
- `text_encoder_lr`: 3e-5 åˆ° 1e-4ï¼ˆå°æ•¸å‡å‹»åˆ†å¸ƒï¼‰

### ç¶²çµ¡æ¶æ§‹
- `network_dim`: 64, 96, 128, 192, 256
- `network_alpha`: 32, 48, 64, 96, 128

### å„ªåŒ–å™¨
- `optimizer_type`: AdamW, AdamW8bit, Lion, Adafactor

### å­¸ç¿’ç‡èª¿åº¦å™¨
- `lr_scheduler`: cosine, cosine_with_restarts, polynomial

### è¨“ç·´è¨­å®š
- `gradient_accumulation_steps`: 1, 2, 4
- `max_train_epochs`: 8, 10, 12, 15
- `lr_warmup_steps`: 50-200ï¼ˆæ­¥é€² 50ï¼‰

## ğŸ“¦ ç³»çµ±çµ„ä»¶

```
scripts/optimization/
â”œâ”€â”€ optuna_hyperparameter_search.py    # ä¸»å„ªåŒ–è…³æœ¬
â”œâ”€â”€ enhanced_metrics.py                 # å¢å¼·è©•ä¼°æŒ‡æ¨™
â”œâ”€â”€ analyze_optuna_results.py          # çµæœåˆ†æå·¥å…·
â””â”€â”€ README_OPTIMIZATION.md             # æœ¬æ–‡ä»¶
```

## ğŸš€ å¿«é€Ÿé–‹å§‹

### æ­¥é©Ÿ 1: ç¢ºèª V6 è¨“ç·´å®Œæˆ

```bash
# æª¢æŸ¥è¨“ç·´ç‹€æ…‹
ls -lh /mnt/data/ai_data/models/lora/luca/iterative_overnight_v6/*.safetensors

# æ‡‰è©²çœ‹åˆ° 6 å€‹ checkpoints (epoch 2, 4, 6, 8, 10, 12)
```

### æ­¥é©Ÿ 2: é‹è¡Œè¶…åƒæ•¸å„ªåŒ–ï¼ˆ30 trialsï¼‰

```bash
# åˆ‡æ›åˆ°é …ç›®ç›®éŒ„
cd /mnt/c/AI_LLM_projects/3d-animation-lora-pipeline

# å•Ÿå‹•å„ªåŒ–ï¼ˆä½¿ç”¨ nohup é¿å…ä¸­æ–·ï¼‰
nohup /home/b0979/.conda/envs/kohya_ss/bin/python \
  scripts/optimization/optuna_hyperparameter_search.py \
  --dataset-config configs/training/luca_human_dataset.toml \
  --output-dir /mnt/data/ai_data/models/lora/luca/optimization_results \
  --study-name luca_pixar_optimization \
  --n-trials 30 \
  --device cuda \
  > /tmp/optuna_optimization.log 2>&1 &

echo "å„ªåŒ–å·²å•Ÿå‹•ï¼ŒPID: $!"
```

### æ­¥é©Ÿ 3: ç›£æ§é€²åº¦

```bash
# å¯¦æ™‚æŸ¥çœ‹æ—¥èªŒ
tail -f /tmp/optuna_optimization.log

# æª¢æŸ¥å·²å®Œæˆçš„ trials
ls /mnt/data/ai_data/models/lora/luca/optimization_results/trial_*/

# æŸ¥çœ‹ Optuna è³‡æ–™åº«
sqlite3 /mnt/data/ai_data/models/lora/luca/optimization_results/optuna_study.db \
  "SELECT number, state, value FROM trials ORDER BY value LIMIT 10;"
```

### æ­¥é©Ÿ 4: åˆ†æçµæœ

```bash
# ç­‰å„ªåŒ–å®Œæˆå¾Œï¼Œé‹è¡Œçµæœåˆ†æ
/home/b0979/.conda/envs/kohya_ss/bin/python \
  scripts/optimization/analyze_optuna_results.py \
  --results-dir /mnt/data/ai_data/models/lora/luca/optimization_results \
  --top-n 10
```

### æ­¥é©Ÿ 5: æŸ¥çœ‹çµæœ

```bash
# æª¢è¦–æœ€ä½³åƒæ•¸
cat /mnt/data/ai_data/models/lora/luca/optimization_results/results/best_parameters.json

# æŸ¥çœ‹è©³ç´°å ±å‘Š
cat /mnt/data/ai_data/models/lora/luca/optimization_results/results/analysis/OPTIMIZATION_REPORT.md

# æŸ¥çœ‹è¦–è¦ºåŒ–åœ–è¡¨
ls /mnt/data/ai_data/models/lora/luca/optimization_results/results/analysis/*.png
```

## ğŸ“Š è¼¸å‡ºçµæ§‹

```
optimization_results/
â”œâ”€â”€ optuna_study.db                          # Optuna è³‡æ–™åº«
â”œâ”€â”€ trial_0001/                              # Trial 1
â”‚   â”œâ”€â”€ params.json                          # è¶…åƒæ•¸
â”‚   â”œâ”€â”€ lora_trial_0001.safetensors         # è¨“ç·´çš„ checkpoint
â”‚   â”œâ”€â”€ training.log                         # è¨“ç·´æ—¥èªŒ
â”‚   â””â”€â”€ evaluation/                          # è©•ä¼°çµæœ
â”‚       â”œâ”€â”€ metrics.json
â”‚       â”œâ”€â”€ EVALUATION_SUMMARY.txt
â”‚       â””â”€â”€ sample_*.png                     # æ¸¬è©¦åœ–ç‰‡
â”œâ”€â”€ trial_0002/
â”‚   â””â”€â”€ ...
â””â”€â”€ results/
    â”œâ”€â”€ best_parameters.json                 # æœ€ä½³è¶…åƒæ•¸
    â”œâ”€â”€ all_trials.json                      # æ‰€æœ‰ trials æ•¸æ“š
    â”œâ”€â”€ optimization_history.png             # å„ªåŒ–æ­·å²åœ–
    â”œâ”€â”€ param_importances.png                # åƒæ•¸é‡è¦æ€§åœ–
    â””â”€â”€ analysis/                            # è©³ç´°åˆ†æ
        â”œâ”€â”€ summary_statistics.json
        â”œâ”€â”€ top_10_trials.csv
        â”œâ”€â”€ score_evolution.png
        â”œâ”€â”€ metrics_comparison.png
        â”œâ”€â”€ parameter_correlation.png
        â”œâ”€â”€ best_training_config.txt         # å¯ç›´æ¥ä½¿ç”¨çš„é…ç½®
        â””â”€â”€ OPTIMIZATION_REPORT.md           # å®Œæ•´å ±å‘Š
```

## ğŸ¯ è©•ä¼°æŒ‡æ¨™èªªæ˜

### Combined Scoreï¼ˆçµ„åˆåˆ†æ•¸ï¼‰

```python
brightness_error = abs(mean_brightness - 0.50)
contrast_error = abs(mean_contrast - 0.20)

brightness_score = brightness_error + 0.5 * std_brightness
contrast_score = contrast_error + 0.5 * std_contrast

combined_score = brightness_score + contrast_score  # è¶Šä½è¶Šå¥½
```

### Pixar Style Scoreï¼ˆPixar é¢¨æ ¼åˆ†æ•¸ï¼‰

åŠ æ¬Šçµ„åˆï¼š
- Brightness in range (0.4-0.6): 30%
- Contrast in range (0.15-0.25): 40%ï¼ˆæœ€é‡è¦ï¼‰
- Saturation in range (0.3-0.5): 20%
- Consistency bonus: 10%

## âš™ï¸ é€²éšè¨­å®š

### èª¿æ•´ Trial æ•¸é‡

```bash
# å¿«é€Ÿæ¸¬è©¦ï¼ˆ10 trialsï¼‰
--n-trials 10

# æ¨™æº–æœç´¢ï¼ˆ30 trialsï¼‰
--n-trials 30

# æ·±åº¦æœç´¢ï¼ˆ50 trialsï¼‰
--n-trials 50
```

### ä½¿ç”¨ä¸åŒçš„ Sampler

ä¿®æ”¹ `optuna_hyperparameter_search.py`ï¼š

```python
# TPE (é è¨­) - é©åˆå¤§å¤šæ•¸æƒ…æ³
sampler = optuna.samplers.TPESampler(seed=42)

# Random - åŸºæº–æ¯”è¼ƒ
sampler = optuna.samplers.RandomSampler(seed=42)

# CMA-ES - é€£çºŒåƒæ•¸å„ªåŒ–
sampler = optuna.samplers.CmaEsSampler(seed=42)
```

### å¤šç›®æ¨™å„ªåŒ–ï¼ˆé€²éšï¼‰

å¦‚æœæƒ³åŒæ™‚å„ªåŒ–å¤šå€‹ç›®æ¨™ï¼š

```python
# ä¿®æ”¹ create_study ç‚ºå¤šç›®æ¨™
study = optuna.create_study(
    directions=["minimize", "minimize"],  # [brightness_score, contrast_score]
    sampler=optuna.samplers.NSGAIISampler(seed=42),
)

# ä¿®æ”¹ objective è¿”å›å¤šå€‹å€¼
return [metrics["brightness_score"], metrics["contrast_score"]]
```

## ğŸ“ˆ é æœŸçµæœ

åŸºæ–¼ V6 çš„ baselineï¼š
- V6 Epoch 2: Brightness 0.444, Contrast 0.190
- V6 Epoch 4: Brightness 0.425, Contrast 0.199

**å„ªåŒ–ç›®æ¨™:**
- Brightness: 0.45-0.55ï¼ˆæ›´æ¥è¿‘ 0.50ï¼‰
- Contrast: 0.18-0.22ï¼ˆæ›´ç©©å®šåœ¨ 0.20 é™„è¿‘ï¼‰
- Consistency: é™ä½ stdï¼ˆbrightness_std < 0.05, contrast_std < 0.02ï¼‰

## ğŸ” å¸¸è¦‹å•é¡Œ

### Q1: å„ªåŒ–éœ€è¦å¤šä¹…ï¼Ÿ

æ¯å€‹ trial åŒ…å«å®Œæ•´è¨“ç·´ï¼ˆ8-15 epochsï¼‰+ è©•ä¼°ï¼š
- å–®å€‹ trial: ~30-60 åˆ†é˜ï¼ˆå–æ±ºæ–¼ epochsï¼‰
- 30 trials: ~15-30 å°æ™‚

**å»ºè­°:** ä½¿ç”¨ `nohup` å’Œ `tmux` é€²è¡Œé•·æ™‚é–“é‹è¡Œ

### Q2: å¦‚ä½•æ¢å¾©ä¸­æ–·çš„å„ªåŒ–ï¼Ÿ

Optuna è‡ªå‹•ä¿å­˜é€²åº¦åˆ° SQLiteï¼š

```bash
# ä½¿ç”¨ç›¸åŒåƒæ•¸é‡æ–°é‹è¡Œå³å¯è‡ªå‹•æ¢å¾©
--study-name luca_pixar_optimization  # ç›¸åŒåç¨±
--storage sqlite:///path/to/optuna_study.db  # ç›¸åŒè³‡æ–™åº«
```

### Q3: å¦‚ä½•æå‰åœæ­¢ä¸è‰¯çš„ trialsï¼Ÿ

å¯¦ç¾ pruning callbackï¼š

```python
# åœ¨ objective å‡½æ•¸ä¸­æ·»åŠ 
if epoch == 2:  # æª¢æŸ¥æ—©æœŸçµæœ
    trial.report(intermediate_score, epoch)
    if trial.should_prune():
        raise optuna.TrialPruned()
```

### Q4: è¨˜æ†¶é«”ä¸è¶³æ€éº¼è¾¦ï¼Ÿ

1. æ¸›å°‘è©•ä¼°æ¨£æœ¬æ•¸é‡: `--num-samples 4`ï¼ˆé è¨­ 8ï¼‰
2. æ¸›å°‘ batch sizeï¼ˆä¿®æ”¹ dataset configï¼‰
3. å•Ÿç”¨ gradient checkpointingï¼ˆå·²å•Ÿç”¨ï¼‰

## ğŸ“ æœ€ä½³å¯¦è¸

### 1. åˆ†éšæ®µå„ªåŒ–

```bash
# éšæ®µ 1: ç²—æœç´¢ï¼ˆ10 trials, 8 epochsï¼‰
--n-trials 10 --max-epochs 8

# éšæ®µ 2: ç²¾ç´°æœç´¢ï¼ˆ20 trials, 12 epochsï¼‰
--n-trials 20 --max-epochs 12

# éšæ®µ 3: é©—è­‰ï¼ˆ5 trials, 15 epochsï¼‰
--n-trials 5 --max-epochs 15
```

### 2. åƒæ•¸ç©ºé–“èª¿æ•´

å¦‚æœåˆæ­¥çµæœé¡¯ç¤ºæŸäº›åƒæ•¸è¡¨ç¾å¥½ï¼š

```python
# ç¸®å°æœç´¢ç¯„åœ
"learning_rate": trial.suggest_float("learning_rate", 8e-5, 1.5e-4, log=True),
"network_dim": trial.suggest_categorical("network_dim", [128, 192, 256]),
```

### 3. ä½¿ç”¨ V6 çµæœä½œç‚º baseline

ä¿å­˜ V6 æœ€ä½³ checkpoint ä½œç‚ºæ¯”è¼ƒåŸºæº–ï¼š

```bash
cp /mnt/data/ai_data/models/lora/luca/iterative_overnight_v6/luca_v6-000004.safetensors \
   /mnt/data/ai_data/models/lora/luca/baseline_v6_epoch4.safetensors
```

## ğŸ“š ç›¸é—œæ–‡æª”

- Optuna å®˜æ–¹æ–‡æª”: https://optuna.readthedocs.io/
- TPE Algorithm: https://optuna.readthedocs.io/en/stable/reference/samplers/generated/optuna.samplers.TPESampler.html
- ITERATIVE_OPTIMIZATION_GUIDE.md: è©³ç´°å„ªåŒ–ç­–ç•¥

## ğŸ†˜ æ•…éšœæ’é™¤

### Error: "CUDA out of memory"

```bash
# è§£æ±ºæ–¹æ¡ˆï¼šæ¸›å°‘ workers å’Œ batch size
--max_data_loader_n_workers 4  # é è¨­ 8
--train_batch_size 1            # å¦‚æœéœ€è¦
```

### Error: "Checkpoint not found"

æª¢æŸ¥è¨“ç·´æ—¥èªŒï¼š

```bash
tail -100 /path/to/trial_XXXX/training.log
```

### Error: "LPIPS/CLIP not available"

é‡æ–°å®‰è£ä¾è³´ï¼š

```bash
conda run -n kohya_ss pip install lpips git+https://github.com/openai/CLIP.git
```

## ğŸ‰ å®Œæˆå¾Œçš„ä¸‹ä¸€æ­¥

1. **é¸æ“‡æœ€ä½³é…ç½®**
   ```bash
   cat results/analysis/best_training_config.txt
   ```

2. **é€²è¡Œå®Œæ•´è¨“ç·´**
   ```bash
   # ä½¿ç”¨æœ€ä½³åƒæ•¸è¨“ç·´å®Œæ•´ç‰ˆæœ¬ï¼ˆå¦‚ 20-30 epochsï¼‰
   ```

3. **A/B æ¸¬è©¦**
   ```bash
   # æ¯”è¼ƒå„ªåŒ–å‰å¾Œçš„ LoRA è³ªé‡
   ```

4. **ç”Ÿç”¢éƒ¨ç½²**
   ```bash
   # å°‡æœ€ä½³ LoRA ç”¨æ–¼å¯¦éš›ç”Ÿæˆ
   ```

---

**ç¥å„ªåŒ–é †åˆ©ï¼** ğŸš€

æœ‰ä»»ä½•å•é¡Œï¼Œè«‹åƒè€ƒ `/mnt/c/AI_LLM_projects/3d-animation-lora-pipeline/docs/guides/ITERATIVE_OPTIMIZATION_GUIDE.md`
