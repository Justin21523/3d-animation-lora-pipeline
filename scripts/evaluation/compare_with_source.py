#!/usr/bin/env python3
"""
è‡ªå‹•åŒ–åˆ†æå·¥å…·ï¼šæ¯”è¼ƒç”Ÿæˆåœ–åƒ vs åŸå§‹é›»å½±æˆªåœ–

é‡åŒ–æŒ‡æ¨™ï¼š
1. å°æ¯”åº¦ (Contrast)
2. å‹•æ…‹ç¯„åœ (Dynamic Range)
3. è‰²å½©é£½å’Œåº¦ (Saturation)
4. äº®åº¦åˆ†ä½ˆ (Brightness Distribution)
5. å…‰ç…§å‡å‹»æ€§ (Lighting Uniformity)
6. è†šè‰²ä¸€è‡´æ€§ (Skin Tone Consistency)
7. SSIM çµæ§‹ç›¸ä¼¼åº¦
8. è‰²å½©ç›´æ–¹åœ–è·é›¢
"""

import argparse
import json
from pathlib import Path
from typing import Dict, List, Tuple
import numpy as np
from PIL import Image, ImageStat
import cv2
from scipy import stats
from collections import defaultdict


class ImageAnalyzer:
    """åœ–åƒåˆ†æå™¨"""

    def __init__(self):
        self.metrics = {}

    def analyze_contrast(self, img_array: np.ndarray) -> Dict:
        """
        åˆ†æå°æ¯”åº¦

        Pixar é›»å½±ç‰¹å¾µï¼š
        - æ¨™æº–å·®: 25-35
        - å‹•æ…‹ç¯„åœ: 150-180

        é«˜å°æ¯”åº¦å•é¡Œï¼š
        - æ¨™æº–å·®: 45-60
        - å‹•æ…‹ç¯„åœ: 200-240
        """
        gray = cv2.cvtColor(img_array, cv2.COLOR_RGB2GRAY)

        return {
            'std_dev': float(np.std(gray)),
            'dynamic_range': int(gray.max() - gray.min()),
            'mean_brightness': float(np.mean(gray)),
            'contrast_level': 'high' if np.std(gray) > 40 else ('medium' if np.std(gray) > 30 else 'low')
        }

    def analyze_color_saturation(self, img_array: np.ndarray) -> Dict:
        """
        åˆ†æè‰²å½©é£½å’Œåº¦

        Pixar ç‰¹å¾µï¼š
        - å¹³è¡¡é£½å’Œåº¦ï¼Œä¸éåº¦é®®è±”
        - è‰²å½©å”èª¿
        """
        hsv = cv2.cvtColor(img_array, cv2.COLOR_RGB2HSV)
        saturation = hsv[:, :, 1]

        return {
            'mean_saturation': float(np.mean(saturation)),
            'saturation_std': float(np.std(saturation)),
            'max_saturation': int(saturation.max()),
            'saturation_level': 'high' if np.mean(saturation) > 100 else ('balanced' if np.mean(saturation) > 60 else 'low')
        }

    def analyze_brightness_distribution(self, img_array: np.ndarray) -> Dict:
        """
        åˆ†æäº®åº¦åˆ†ä½ˆ

        æª¢æŸ¥æ˜¯å¦æœ‰éæš—æˆ–éäº®å€åŸŸ
        """
        gray = cv2.cvtColor(img_array, cv2.COLOR_RGB2GRAY)
        hist, _ = np.histogram(gray, bins=256, range=(0, 256))

        # è¨ˆç®—å„å€åŸŸçš„åƒç´ æ¯”ä¾‹
        shadows = np.sum(hist[:85]) / gray.size  # 0-85: é™°å½±
        midtones = np.sum(hist[85:170]) / gray.size  # 85-170: ä¸­é–“èª¿
        highlights = np.sum(hist[170:]) / gray.size  # 170-255: é«˜å…‰

        return {
            'shadows_ratio': float(shadows),
            'midtones_ratio': float(midtones),
            'highlights_ratio': float(highlights),
            'skewness': float(stats.skew(hist)),
            'kurtosis': float(stats.kurtosis(hist))
        }

    def analyze_lighting_uniformity(self, img_array: np.ndarray) -> Dict:
        """
        åˆ†æå…‰ç…§å‡å‹»æ€§

        Pixar ç‰¹å¾µï¼š
        - å…¨å±€ç…§æ˜å‡å‹»
        - å°‘é‡æ–¹å‘æ€§å…‰æº
        - æ²’æœ‰å¼·çƒˆç†±é»
        """
        gray = cv2.cvtColor(img_array, cv2.COLOR_RGB2GRAY)

        # åˆ†å‰²æˆå¤šå€‹å€åŸŸä¸¦è¨ˆç®—æ¨™æº–å·®
        h, w = gray.shape
        grid_size = 4
        regions = []

        for i in range(grid_size):
            for j in range(grid_size):
                y1 = i * h // grid_size
                y2 = (i + 1) * h // grid_size
                x1 = j * w // grid_size
                x2 = (j + 1) * w // grid_size
                region = gray[y1:y2, x1:x2]
                regions.append(np.mean(region))

        # å€åŸŸé–“äº®åº¦å·®ç•°
        region_std = np.std(regions)

        return {
            'region_brightness_std': float(region_std),
            'uniformity_score': float(100 - min(region_std, 100)),  # 0-100
            'uniformity_level': 'high' if region_std < 20 else ('medium' if region_std < 40 else 'low')
        }

    def analyze_skin_tone_consistency(self, img_array: np.ndarray, mask: np.ndarray = None) -> Dict:
        """
        åˆ†æè†šè‰²ä¸€è‡´æ€§

        Pixar ç‰¹å¾µï¼š
        - è†šè‰²å‡å‹»
        - æ²’æœ‰éåº¦é«˜å…‰æˆ–é™°å½±
        """
        # ç°¡åŒ–ç‰ˆï¼šæª¢æ¸¬è†šè‰²ç¯„åœçš„ä¸€è‡´æ€§
        # çœŸå¯¦æ‡‰ç”¨éœ€è¦äººè‡‰æª¢æ¸¬

        # YCrCb è‰²å½©ç©ºé–“å°è†šè‰²æª¢æ¸¬æ›´æœ‰æ•ˆ
        ycrcb = cv2.cvtColor(img_array, cv2.COLOR_RGB2YCrCb)

        # è†šè‰²ç¯„åœï¼ˆç²—ç•¥ï¼‰
        lower = np.array([0, 133, 77], dtype=np.uint8)
        upper = np.array([255, 173, 127], dtype=np.uint8)

        skin_mask = cv2.inRange(ycrcb, lower, upper)
        skin_pixels = img_array[skin_mask > 0]

        if len(skin_pixels) == 0:
            return {
                'detected': False,
                'error': 'No skin tone detected'
            }

        return {
            'detected': True,
            'mean_color': [int(x) for x in np.mean(skin_pixels, axis=0)],
            'color_std': [float(x) for x in np.std(skin_pixels, axis=0)],
            'consistency_score': float(100 - min(np.mean(np.std(skin_pixels, axis=0)), 100))
        }

    def compare_images(self, img1: np.ndarray, img2: np.ndarray) -> Dict:
        """
        æ¯”è¼ƒå…©å¼µåœ–åƒçš„çµæ§‹ç›¸ä¼¼åº¦å’Œè‰²å½©å·®ç•°
        """
        from skimage.metrics import structural_similarity as ssim

        # ç¢ºä¿å…©å¼µåœ–åƒå°ºå¯¸ç›¸åŒ
        if img1.shape != img2.shape:
            h, w = min(img1.shape[0], img2.shape[0]), min(img1.shape[1], img2.shape[1])
            img1 = cv2.resize(img1, (w, h))
            img2 = cv2.resize(img2, (w, h))

        # è½‰ç°éšè¨ˆç®— SSIM
        gray1 = cv2.cvtColor(img1, cv2.COLOR_RGB2GRAY)
        gray2 = cv2.cvtColor(img2, cv2.COLOR_RGB2GRAY)

        ssim_score = ssim(gray1, gray2)

        # è‰²å½©ç›´æ–¹åœ–è·é›¢
        hist1 = [cv2.calcHist([img1], [i], None, [256], [0, 256]) for i in range(3)]
        hist2 = [cv2.calcHist([img2], [i], None, [256], [0, 256]) for i in range(3)]

        hist_distances = [
            cv2.compareHist(h1, h2, cv2.HISTCMP_BHATTACHARYYA)
            for h1, h2 in zip(hist1, hist2)
        ]

        return {
            'ssim': float(ssim_score),
            'hist_distance_r': float(hist_distances[0]),
            'hist_distance_g': float(hist_distances[1]),
            'hist_distance_b': float(hist_distances[2]),
            'avg_hist_distance': float(np.mean(hist_distances))
        }

    def full_analysis(self, image_path: Path) -> Dict:
        """å®Œæ•´åˆ†æå–®å¼µåœ–åƒ"""
        img = Image.open(image_path).convert('RGB')
        img_array = np.array(img)

        return {
            'image_path': str(image_path),
            'resolution': f"{img.width}x{img.height}",
            'contrast': self.analyze_contrast(img_array),
            'saturation': self.analyze_color_saturation(img_array),
            'brightness': self.analyze_brightness_distribution(img_array),
            'lighting': self.analyze_lighting_uniformity(img_array),
            'skin_tone': self.analyze_skin_tone_consistency(img_array)
        }


def analyze_dataset(
    source_dir: Path,
    generated_dir: Path = None,
    output_path: Path = None,
    sample_size: int = 50
) -> Dict:
    """
    åˆ†ææ•´å€‹æ•¸æ“šé›†

    Args:
        source_dir: åŸå§‹é›»å½±æˆªåœ–ç›®éŒ„
        generated_dir: ç”Ÿæˆåœ–åƒç›®éŒ„ï¼ˆå¯é¸ï¼‰
        output_path: è¼¸å‡º JSON å ±å‘Šè·¯å¾‘
        sample_size: åˆ†æçš„æ¨£æœ¬æ•¸é‡
    """
    analyzer = ImageAnalyzer()

    # åˆ†æåŸå§‹åœ–åƒ
    print(f"ğŸ“Š åˆ†æåŸå§‹é›»å½±æˆªåœ–...")
    source_images = list(source_dir.glob("*.png"))[:sample_size]

    source_results = []
    for img_path in source_images:
        print(f"  åˆ†æ: {img_path.name}")
        result = analyzer.full_analysis(img_path)
        source_results.append(result)

    # çµ±è¨ˆåŸå§‹åœ–åƒçš„å¹³å‡å€¼
    source_stats = aggregate_statistics(source_results)

    report = {
        'source_images': {
            'count': len(source_results),
            'directory': str(source_dir),
            'statistics': source_stats,
            'samples': source_results[:5]  # ä¿å­˜å‰5å€‹æ¨£æœ¬
        }
    }

    # å¦‚æœæœ‰ç”Ÿæˆåœ–åƒï¼Œä¹Ÿåˆ†æä¸¦å°æ¯”
    if generated_dir and generated_dir.exists():
        print(f"\nğŸ“Š åˆ†æç”Ÿæˆåœ–åƒ...")
        generated_images = list(generated_dir.glob("*.png"))[:sample_size]

        generated_results = []
        for img_path in generated_images:
            print(f"  åˆ†æ: {img_path.name}")
            result = analyzer.full_analysis(img_path)
            generated_results.append(result)

        generated_stats = aggregate_statistics(generated_results)

        report['generated_images'] = {
            'count': len(generated_results),
            'directory': str(generated_dir),
            'statistics': generated_stats,
            'samples': generated_results[:5]
        }

        # å°æ¯”åˆ†æ
        print(f"\nğŸ“Š å°æ¯”åˆ†æ...")
        report['comparison'] = compare_statistics(source_stats, generated_stats)

    # ç”Ÿæˆè¨ºæ–·å»ºè­°
    report['diagnosis'] = generate_diagnosis(report)

    # ä¿å­˜å ±å‘Š
    if output_path:
        output_path.parent.mkdir(parents=True, exist_ok=True)
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
        print(f"\nâœ… å ±å‘Šå·²ä¿å­˜: {output_path}")

    return report


def aggregate_statistics(results: List[Dict]) -> Dict:
    """èšåˆå¤šå€‹åœ–åƒçš„çµ±è¨ˆæ•¸æ“š"""
    stats = defaultdict(list)

    for result in results:
        # å°æ¯”åº¦
        stats['contrast_std'].append(result['contrast']['std_dev'])
        stats['dynamic_range'].append(result['contrast']['dynamic_range'])

        # é£½å’Œåº¦
        stats['saturation_mean'].append(result['saturation']['mean_saturation'])

        # å…‰ç…§å‡å‹»æ€§
        stats['lighting_uniformity'].append(result['lighting']['uniformity_score'])

        # äº®åº¦åˆ†ä½ˆ
        stats['shadows_ratio'].append(result['brightness']['shadows_ratio'])
        stats['midtones_ratio'].append(result['brightness']['midtones_ratio'])
        stats['highlights_ratio'].append(result['brightness']['highlights_ratio'])

    # è¨ˆç®—å¹³å‡å€¼å’Œæ¨™æº–å·®
    aggregated = {}
    for key, values in stats.items():
        aggregated[key] = {
            'mean': float(np.mean(values)),
            'std': float(np.std(values)),
            'min': float(np.min(values)),
            'max': float(np.max(values))
        }

    return aggregated


def compare_statistics(source_stats: Dict, generated_stats: Dict) -> Dict:
    """å°æ¯”åŸå§‹å’Œç”Ÿæˆåœ–åƒçš„çµ±è¨ˆæ•¸æ“š"""
    comparison = {}

    for key in source_stats:
        if key in generated_stats:
            source_mean = source_stats[key]['mean']
            generated_mean = generated_stats[key]['mean']

            diff = generated_mean - source_mean
            diff_percent = (diff / source_mean * 100) if source_mean != 0 else 0

            comparison[key] = {
                'source': source_mean,
                'generated': generated_mean,
                'difference': float(diff),
                'difference_percent': float(diff_percent)
            }

    return comparison


def generate_diagnosis(report: Dict) -> Dict:
    """æ ¹æ“šæ•¸æ“šç”Ÿæˆè¨ºæ–·å»ºè­°"""
    diagnosis = {
        'issues': [],
        'recommendations': []
    }

    if 'comparison' in report:
        comp = report['comparison']

        # æª¢æŸ¥å°æ¯”åº¦
        if 'contrast_std' in comp:
            diff = comp['contrast_std']['difference_percent']
            if diff > 20:
                diagnosis['issues'].append({
                    'severity': 'high',
                    'category': 'contrast',
                    'description': f'å°æ¯”åº¦éé«˜ï¼ˆé«˜å‡ºåŸç‰‡ {diff:.1f}%ï¼‰',
                    'metric': f"åŸç‰‡: {comp['contrast_std']['source']:.1f}, ç”Ÿæˆ: {comp['contrast_std']['generated']:.1f}"
                })
                diagnosis['recommendations'].append({
                    'category': 'caption',
                    'action': 'æ·»åŠ  "low contrast, even illumination" åˆ° caption'
                })
                diagnosis['recommendations'].append({
                    'category': 'training',
                    'action': 'é™ä½å­¸ç¿’ç‡æˆ–å¢åŠ  text encoder æ¬Šé‡'
                })

        # æª¢æŸ¥å…‰ç…§å‡å‹»æ€§
        if 'lighting_uniformity' in comp:
            diff = comp['lighting_uniformity']['difference_percent']
            if diff < -20:  # ç”Ÿæˆåœ–åƒçš„å‡å‹»æ€§æ›´å·®
                diagnosis['issues'].append({
                    'severity': 'high',
                    'category': 'lighting',
                    'description': f'å…‰ç…§ä¸å‡å‹»ï¼ˆå·®æ–¼åŸç‰‡ {abs(diff):.1f}%ï¼‰',
                    'metric': f"åŸç‰‡: {comp['lighting_uniformity']['source']:.1f}, ç”Ÿæˆ: {comp['lighting_uniformity']['generated']:.1f}"
                })
                diagnosis['recommendations'].append({
                    'category': 'caption',
                    'action': 'æ·»åŠ  "pixar uniform lighting, no harsh shadows"'
                })

        # æª¢æŸ¥é£½å’Œåº¦
        if 'saturation_mean' in comp:
            diff = comp['saturation_mean']['difference_percent']
            if diff > 15:
                diagnosis['issues'].append({
                    'severity': 'medium',
                    'category': 'saturation',
                    'description': f'è‰²å½©éåº¦é£½å’Œï¼ˆé«˜å‡ºåŸç‰‡ {diff:.1f}%ï¼‰',
                    'metric': f"åŸç‰‡: {comp['saturation_mean']['source']:.1f}, ç”Ÿæˆ: {comp['saturation_mean']['generated']:.1f}"
                })
                diagnosis['recommendations'].append({
                    'category': 'caption',
                    'action': 'æ·»åŠ  "balanced saturation, film color grading"'
                })

    return diagnosis


def print_report_summary(report: Dict):
    """æ‰“å°å ±å‘Šæ‘˜è¦"""
    print("\n" + "="*70)
    print("ğŸ“Š åœ–åƒè³ªé‡åˆ†æå ±å‘Š")
    print("="*70)

    if 'source_images' in report:
        print(f"\nåŸå§‹é›»å½±æˆªåœ–çµ±è¨ˆ ({report['source_images']['count']} å¼µ):")
        stats = report['source_images']['statistics']
        print(f"  å°æ¯”åº¦æ¨™æº–å·®: {stats['contrast_std']['mean']:.1f} Â± {stats['contrast_std']['std']:.1f}")
        print(f"  å…‰ç…§å‡å‹»æ€§: {stats['lighting_uniformity']['mean']:.1f}/100")
        print(f"  è‰²å½©é£½å’Œåº¦: {stats['saturation_mean']['mean']:.1f}/255")

    if 'generated_images' in report:
        print(f"\nç”Ÿæˆåœ–åƒçµ±è¨ˆ ({report['generated_images']['count']} å¼µ):")
        stats = report['generated_images']['statistics']
        print(f"  å°æ¯”åº¦æ¨™æº–å·®: {stats['contrast_std']['mean']:.1f} Â± {stats['contrast_std']['std']:.1f}")
        print(f"  å…‰ç…§å‡å‹»æ€§: {stats['lighting_uniformity']['mean']:.1f}/100")
        print(f"  è‰²å½©é£½å’Œåº¦: {stats['saturation_mean']['mean']:.1f}/255")

    if 'comparison' in report:
        print(f"\nğŸ“ˆ å°æ¯”åˆ†æ:")
        comp = report['comparison']
        if 'contrast_std' in comp:
            print(f"  å°æ¯”åº¦å·®ç•°: {comp['contrast_std']['difference_percent']:+.1f}%")
        if 'lighting_uniformity' in comp:
            print(f"  å…‰ç…§å‡å‹»æ€§å·®ç•°: {comp['lighting_uniformity']['difference_percent']:+.1f}%")
        if 'saturation_mean' in comp:
            print(f"  é£½å’Œåº¦å·®ç•°: {comp['saturation_mean']['difference_percent']:+.1f}%")

    if 'diagnosis' in report:
        diagnosis = report['diagnosis']

        if diagnosis['issues']:
            print(f"\nâš ï¸  ç™¼ç¾ {len(diagnosis['issues'])} å€‹å•é¡Œ:")
            for issue in diagnosis['issues']:
                print(f"  [{issue['severity'].upper()}] {issue['description']}")
                print(f"    æ•¸æ“š: {issue['metric']}")

        if diagnosis['recommendations']:
            print(f"\nğŸ’¡ æ”¹å–„å»ºè­° ({len(diagnosis['recommendations'])} æ¢):")
            for rec in diagnosis['recommendations']:
                print(f"  [{rec['category']}] {rec['action']}")

    print("\n" + "="*70)


def main():
    parser = argparse.ArgumentParser(
        description="è‡ªå‹•åŒ–åˆ†æå·¥å…·ï¼šæ¯”è¼ƒç”Ÿæˆåœ–åƒ vs åŸå§‹é›»å½±æˆªåœ–"
    )
    parser.add_argument(
        "--source",
        type=Path,
        required=True,
        help="åŸå§‹é›»å½±æˆªåœ–ç›®éŒ„"
    )
    parser.add_argument(
        "--generated",
        type=Path,
        default=None,
        help="ç”Ÿæˆåœ–åƒç›®éŒ„ï¼ˆå¯é¸ï¼‰"
    )
    parser.add_argument(
        "--output",
        type=Path,
        default=None,
        help="è¼¸å‡º JSON å ±å‘Šè·¯å¾‘"
    )
    parser.add_argument(
        "--sample-size",
        type=int,
        default=50,
        help="åˆ†æçš„æ¨£æœ¬æ•¸é‡"
    )

    args = parser.parse_args()

    if not args.source.exists():
        print(f"éŒ¯èª¤ï¼šåŸå§‹åœ–åƒç›®éŒ„ä¸å­˜åœ¨: {args.source}")
        return 1

    # é‹è¡Œåˆ†æ
    report = analyze_dataset(
        source_dir=args.source,
        generated_dir=args.generated,
        output_path=args.output,
        sample_size=args.sample_size
    )

    # æ‰“å°æ‘˜è¦
    print_report_summary(report)

    return 0


if __name__ == "__main__":
    exit(main())
