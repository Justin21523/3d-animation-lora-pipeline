#!/usr/bin/env python3
"""
SOTA (State-of-the-Art) è³ªé‡åˆ†æå™¨

æ•´åˆæœ€å…ˆé€²çš„ AI æ¨¡å‹ï¼š
1. CLIP/SigLIP - èªç¾©ç›¸ä¼¼åº¦å’Œé¢¨æ ¼ä¸€è‡´æ€§
2. MUSIQ/NIMA - ç¾å­¸è³ªé‡è©•åˆ†
3. LPIPS - æ„ŸçŸ¥æå¤±
4. InsightFace (ArcFace) - äººè‡‰è­˜åˆ¥å’Œä¸€è‡´æ€§
5. BRISQUE - ç„¡åƒè€ƒåœ–åƒè³ªé‡è©•ä¼°
6. FID - Frechet Inception Distance
7. InternVL2/Qwen2-VL - è¦–è¦ºèªè¨€ç†è§£
8. è‡ªå®šç¾©å…‰ç…§åˆ†ææ¨¡å‹
"""

import argparse
import json
from pathlib import Path
from typing import Dict, List, Tuple, Optional
import numpy as np
from PIL import Image
import torch
import torch.nn.functional as F
from tqdm import tqdm
import warnings
warnings.filterwarnings('ignore')


class SOTAQualityAnalyzer:
    """SOTA è³ªé‡åˆ†æå™¨"""

    def __init__(self, device: str = 'cuda'):
        self.device = device
        self.models = {}
        print("ğŸš€ åˆå§‹åŒ– SOTA åˆ†ææ¨¡å‹...")

    def load_clip_model(self):
        """åŠ è¼‰ CLIP æ¨¡å‹ç”¨æ–¼èªç¾©åˆ†æ"""
        if 'clip' not in self.models:
            print("  ğŸ“¦ åŠ è¼‰ CLIP æ¨¡å‹...")
            import clip
            model, preprocess = clip.load("ViT-L/14", device=self.device)
            self.models['clip'] = model
            self.models['clip_preprocess'] = preprocess
            print("    âœ“ CLIP å·²åŠ è¼‰")
        return self.models['clip'], self.models['clip_preprocess']

    def load_musiq_model(self):
        """åŠ è¼‰ MUSIQ ç¾å­¸è³ªé‡è©•åˆ†æ¨¡å‹"""
        if 'musiq' not in self.models:
            print("  ğŸ“¦ åŠ è¼‰ MUSIQ ç¾å­¸è©•åˆ†æ¨¡å‹...")
            try:
                from transformers import AutoImageProcessor, AutoModelForImageClassification
                processor = AutoImageProcessor.from_pretrained("google/musiq-ava-ckpt-300000")
                model = AutoModelForImageClassification.from_pretrained("google/musiq-ava-ckpt-300000")
                model = model.to(self.device).eval()
                self.models['musiq'] = model
                self.models['musiq_processor'] = processor
                print("    âœ“ MUSIQ å·²åŠ è¼‰")
            except Exception as e:
                print(f"    âš ï¸  MUSIQ åŠ è¼‰å¤±æ•—: {e}")
                self.models['musiq'] = None
        return self.models.get('musiq'), self.models.get('musiq_processor')

    def load_lpips_model(self):
        """åŠ è¼‰ LPIPS æ„ŸçŸ¥æå¤±æ¨¡å‹"""
        if 'lpips' not in self.models:
            print("  ğŸ“¦ åŠ è¼‰ LPIPS æ„ŸçŸ¥æå¤±æ¨¡å‹...")
            try:
                import lpips
                model = lpips.LPIPS(net='alex').to(self.device)
                self.models['lpips'] = model
                print("    âœ“ LPIPS å·²åŠ è¼‰")
            except Exception as e:
                print(f"    âš ï¸  LPIPS åŠ è¼‰å¤±æ•—: {e}")
                print("    ğŸ’¡ å®‰è£: pip install lpips")
                self.models['lpips'] = None
        return self.models.get('lpips')

    def load_insightface_model(self):
        """åŠ è¼‰ InsightFace äººè‡‰åˆ†ææ¨¡å‹"""
        if 'insightface' not in self.models:
            print("  ğŸ“¦ åŠ è¼‰ InsightFace äººè‡‰åˆ†æ...")
            try:
                from insightface.app import FaceAnalysis
                app = FaceAnalysis(providers=['CUDAExecutionProvider', 'CPUExecutionProvider'])
                app.prepare(ctx_id=0 if self.device == 'cuda' else -1, det_size=(640, 640))
                self.models['insightface'] = app
                print("    âœ“ InsightFace å·²åŠ è¼‰")
            except Exception as e:
                print(f"    âš ï¸  InsightFace åŠ è¼‰å¤±æ•—: {e}")
                print("    ğŸ’¡ å®‰è£: pip install insightface")
                self.models['insightface'] = None
        return self.models.get('insightface')

    def load_brisque_model(self):
        """åŠ è¼‰ BRISQUE ç„¡åƒè€ƒè³ªé‡è©•ä¼°"""
        if 'brisque' not in self.models:
            print("  ğŸ“¦ åŠ è¼‰ BRISQUE è³ªé‡è©•ä¼°...")
            try:
                import cv2
                # BRISQUE å…§å»ºæ–¼ OpenCV
                self.models['brisque'] = True
                print("    âœ“ BRISQUE å·²åŠ è¼‰")
            except Exception as e:
                print(f"    âš ï¸  BRISQUE ä¸å¯ç”¨: {e}")
                self.models['brisque'] = None
        return self.models.get('brisque')

    def load_inception_model(self):
        """åŠ è¼‰ Inception v3 ç”¨æ–¼ FID è¨ˆç®—"""
        if 'inception' not in self.models:
            print("  ğŸ“¦ åŠ è¼‰ Inception v3 (FID)...")
            try:
                from torchvision.models import inception_v3
                model = inception_v3(pretrained=True, transform_input=False)
                model = model.to(self.device).eval()
                self.models['inception'] = model
                print("    âœ“ Inception v3 å·²åŠ è¼‰")
            except Exception as e:
                print(f"    âš ï¸  Inception åŠ è¼‰å¤±æ•—: {e}")
                self.models['inception'] = None
        return self.models.get('inception')

    # ============= åˆ†ææ–¹æ³• =============

    def analyze_semantic_similarity(self, img1: Image.Image, img2: Image.Image) -> Dict:
        """
        ä½¿ç”¨ CLIP åˆ†æèªç¾©ç›¸ä¼¼åº¦

        è©•ä¼°å…©å¼µåœ–åƒåœ¨èªç¾©ç©ºé–“çš„ç›¸ä¼¼æ€§
        """
        clip_model, preprocess = self.load_clip_model()

        with torch.no_grad():
            img1_tensor = preprocess(img1).unsqueeze(0).to(self.device)
            img2_tensor = preprocess(img2).unsqueeze(0).to(self.device)

            feat1 = clip_model.encode_image(img1_tensor)
            feat2 = clip_model.encode_image(img2_tensor)

            # æ­¸ä¸€åŒ–
            feat1 = F.normalize(feat1, dim=-1)
            feat2 = F.normalize(feat2, dim=-1)

            # é¤˜å¼¦ç›¸ä¼¼åº¦
            similarity = (feat1 @ feat2.T).item()

        return {
            'clip_similarity': float(similarity),
            'semantic_consistency': 'high' if similarity > 0.85 else ('medium' if similarity > 0.75 else 'low')
        }

    def analyze_aesthetic_quality(self, image: Image.Image) -> Dict:
        """
        ä½¿ç”¨ MUSIQ åˆ†æç¾å­¸è³ªé‡

        è¿”å› 1-10 çš„ç¾å­¸è©•åˆ†
        """
        musiq_model, processor = self.load_musiq_model()

        if musiq_model is None:
            return {'error': 'MUSIQ model not available'}

        with torch.no_grad():
            inputs = processor(images=image, return_tensors="pt")
            inputs = {k: v.to(self.device) for k, v in inputs.items()}

            outputs = musiq_model(**inputs)
            score = outputs.logits.item()

        return {
            'aesthetic_score': float(score),
            'aesthetic_rating': 'excellent' if score > 7 else ('good' if score > 5 else 'poor')
        }

    def analyze_perceptual_distance(self, img1: Image.Image, img2: Image.Image) -> Dict:
        """
        ä½¿ç”¨ LPIPS è¨ˆç®—æ„ŸçŸ¥è·é›¢

        è¶Šä½è¡¨ç¤ºè¦–è¦ºä¸Šè¶Šç›¸ä¼¼ï¼ˆ0-1ï¼‰
        """
        lpips_model = self.load_lpips_model()

        if lpips_model is None:
            return {'error': 'LPIPS model not available'}

        # è½‰æ›ç‚º tensor [-1, 1]
        from torchvision import transforms
        transform = transforms.Compose([
            transforms.Resize((256, 256)),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])
        ])

        img1_tensor = transform(img1).unsqueeze(0).to(self.device)
        img2_tensor = transform(img2).unsqueeze(0).to(self.device)

        with torch.no_grad():
            distance = lpips_model(img1_tensor, img2_tensor).item()

        return {
            'lpips_distance': float(distance),
            'perceptual_similarity': 'high' if distance < 0.15 else ('medium' if distance < 0.30 else 'low')
        }

    def analyze_face_consistency(self, images: List[Image.Image]) -> Dict:
        """
        ä½¿ç”¨ InsightFace åˆ†æäººè‡‰ä¸€è‡´æ€§

        æª¢æ¸¬å¤šå¼µåœ–åƒä¸­çš„äººè‡‰æ˜¯å¦ç‚ºåŒä¸€å€‹äºº
        """
        insightface_app = self.load_insightface_model()

        if insightface_app is None:
            return {'error': 'InsightFace model not available'}

        embeddings = []
        face_counts = []

        for img in images:
            import cv2
            img_cv = cv2.cvtColor(np.array(img), cv2.COLOR_RGB2BGR)
            faces = insightface_app.get(img_cv)

            face_counts.append(len(faces))

            if len(faces) > 0:
                # å–æœ€å¤§çš„äººè‡‰
                largest_face = max(faces, key=lambda f: f.bbox[2] * f.bbox[3])
                embeddings.append(largest_face.embedding)

        if len(embeddings) < 2:
            return {
                'error': 'Not enough faces detected',
                'face_detection_rate': len(embeddings) / len(images)
            }

        # è¨ˆç®—åµŒå…¥å‘é‡é–“çš„ç›¸ä¼¼åº¦
        embeddings = np.array(embeddings)
        similarities = []

        for i in range(len(embeddings)):
            for j in range(i+1, len(embeddings)):
                sim = np.dot(embeddings[i], embeddings[j]) / (
                    np.linalg.norm(embeddings[i]) * np.linalg.norm(embeddings[j])
                )
                similarities.append(sim)

        avg_similarity = np.mean(similarities)

        return {
            'face_detection_rate': len(embeddings) / len(images),
            'avg_face_similarity': float(avg_similarity),
            'face_consistency': 'high' if avg_similarity > 0.7 else ('medium' if avg_similarity > 0.5 else 'low'),
            'faces_per_image': {
                'mean': float(np.mean(face_counts)),
                'min': int(np.min(face_counts)),
                'max': int(np.max(face_counts))
            }
        }

    def analyze_image_quality(self, image: Image.Image) -> Dict:
        """
        ä½¿ç”¨ BRISQUE åˆ†æåœ–åƒè³ªé‡ï¼ˆç„¡åƒè€ƒï¼‰

        åˆ†æ•¸è¶Šä½è¶Šå¥½ï¼ˆ0-100ï¼‰
        """
        import cv2

        img_cv = cv2.cvtColor(np.array(image), cv2.COLOR_RGB2BGR)
        gray = cv2.cvtColor(img_cv, cv2.COLOR_BGR2GRAY)

        try:
            # BRISQUE åˆ†æ•¸
            brisque = cv2.quality.QualityBRISQUE_compute(gray, "brisque_model_live.yml", "brisque_range_live.yml")
            score = brisque[0]
        except:
            # å¦‚æœæ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨ï¼Œä½¿ç”¨ç°¡åŒ–ç‰ˆæœ¬
            score = self._compute_simple_quality(gray)

        return {
            'brisque_score': float(score),
            'image_quality': 'excellent' if score < 20 else ('good' if score < 40 else 'poor')
        }

    def _compute_simple_quality(self, gray_img: np.ndarray) -> float:
        """ç°¡åŒ–çš„åœ–åƒè³ªé‡è©•ä¼°"""
        # åŸºæ–¼æ¸…æ™°åº¦å’Œå™ªè²çš„ç°¡å–®ä¼°è¨ˆ
        import cv2

        # Laplacian æ–¹å·®ï¼ˆæ¸…æ™°åº¦ï¼‰
        laplacian = cv2.Laplacian(gray_img, cv2.CV_64F)
        sharpness = laplacian.var()

        # å™ªè²ä¼°è¨ˆï¼ˆé«˜é »èƒ½é‡ï¼‰
        noise = np.std(gray_img)

        # ç¶œåˆè©•åˆ†ï¼ˆæ­¸ä¸€åŒ–åˆ° 0-100ï¼‰
        score = 100 - min(sharpness / 10, 100) + noise / 2

        return float(score)

    def compute_fid(self, real_images: List[Image.Image], fake_images: List[Image.Image]) -> Dict:
        """
        è¨ˆç®— Frechet Inception Distance (FID)

        è©•ä¼°ç”Ÿæˆåœ–åƒåˆ†ä½ˆèˆ‡çœŸå¯¦åœ–åƒåˆ†ä½ˆçš„è·é›¢
        """
        inception_model = self.load_inception_model()

        if inception_model is None:
            return {'error': 'Inception model not available'}

        from torchvision import transforms
        transform = transforms.Compose([
            transforms.Resize(299),
            transforms.CenterCrop(299),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
        ])

        def get_activations(images):
            activations = []
            with torch.no_grad():
                for img in images:
                    img_tensor = transform(img).unsqueeze(0).to(self.device)
                    pred = inception_model(img_tensor)
                    activations.append(pred.cpu().numpy())
            return np.concatenate(activations, axis=0)

        real_act = get_activations(real_images)
        fake_act = get_activations(fake_images)

        # è¨ˆç®—å‡å€¼å’Œå”æ–¹å·®
        mu_real, sigma_real = real_act.mean(axis=0), np.cov(real_act, rowvar=False)
        mu_fake, sigma_fake = fake_act.mean(axis=0), np.cov(fake_act, rowvar=False)

        # FID å…¬å¼
        from scipy import linalg
        diff = mu_real - mu_fake
        covmean, _ = linalg.sqrtm(sigma_real.dot(sigma_fake), disp=False)

        if np.iscomplexobj(covmean):
            covmean = covmean.real

        fid = diff.dot(diff) + np.trace(sigma_real + sigma_fake - 2*covmean)

        return {
            'fid_score': float(fid),
            'distribution_similarity': 'high' if fid < 50 else ('medium' if fid < 150 else 'low')
        }

    def analyze_lighting_properties(self, image: Image.Image) -> Dict:
        """
        åˆ†æå…‰ç…§å±¬æ€§

        ä½¿ç”¨è¨ˆç®—æ©Ÿè¦–è¦ºæŠ€è¡“åˆ†æï¼š
        - å°æ¯”åº¦
        - å‹•æ…‹ç¯„åœ
        - å…‰ç…§å‡å‹»æ€§
        - é™°å½±åˆ†ä½ˆ
        """
        import cv2

        img_cv = cv2.cvtColor(np.array(image), cv2.COLOR_RGB2BGR)
        gray = cv2.cvtColor(img_cv, cv2.COLOR_BGR2GRAY)

        # å°æ¯”åº¦ï¼ˆæ¨™æº–å·®ï¼‰
        contrast = float(gray.std())

        # å‹•æ…‹ç¯„åœ
        dynamic_range = int(gray.max() - gray.min())

        # å…‰ç…§å‡å‹»æ€§ï¼ˆå€åŸŸäº®åº¦æ–¹å·®ï¼‰
        h, w = gray.shape
        regions = []
        for i in range(4):
            for j in range(4):
                region = gray[i*h//4:(i+1)*h//4, j*w//4:(j+1)*w//4]
                regions.append(region.mean())

        uniformity = 100 - min(np.std(regions), 100)

        # é™°å½±/é«˜å…‰æ¯”ä¾‹
        hist, _ = np.histogram(gray, bins=256, range=(0, 256))
        shadows = np.sum(hist[:85]) / gray.size
        midtones = np.sum(hist[85:170]) / gray.size
        highlights = np.sum(hist[170:]) / gray.size

        # åˆ¤æ–·æ˜¯å¦ç¬¦åˆ Pixar ç‰¹å¾µ
        is_low_contrast = contrast < 40
        is_uniform = uniformity > 70
        is_balanced = 0.3 < midtones < 0.7

        pixar_score = (
            (1 if is_low_contrast else 0) * 0.4 +
            (1 if is_uniform else 0) * 0.4 +
            (1 if is_balanced else 0) * 0.2
        ) * 100

        return {
            'contrast_std': contrast,
            'dynamic_range': dynamic_range,
            'lighting_uniformity': float(uniformity),
            'shadow_ratio': float(shadows),
            'midtone_ratio': float(midtones),
            'highlight_ratio': float(highlights),
            'is_low_contrast': is_low_contrast,
            'is_uniform_lighting': is_uniform,
            'is_balanced_exposure': is_balanced,
            'pixar_lighting_score': float(pixar_score),
            'lighting_assessment': 'pixar-like' if pixar_score > 70 else ('acceptable' if pixar_score > 50 else 'needs_improvement')
        }

    # ============= ç¶œåˆåˆ†æ =============

    def comprehensive_analysis(
        self,
        generated_images: List[Path],
        source_images: List[Path],
        output_path: Path = None
    ) -> Dict:
        """
        å…¨é¢åˆ†æç”Ÿæˆåœ–åƒè³ªé‡

        å°æ¯”åŸå§‹åœ–åƒå’Œç”Ÿæˆåœ–åƒï¼Œè¼¸å‡ºè©³ç´°å ±å‘Š
        """
        print("\n" + "="*70)
        print("ğŸ¯ SOTA è³ªé‡åˆ†æé–‹å§‹")
        print("="*70)

        # åŠ è¼‰åœ–åƒ
        print(f"\nğŸ“‚ åŠ è¼‰åœ–åƒ...")
        gen_imgs = [Image.open(p).convert('RGB') for p in generated_images[:50]]
        src_imgs = [Image.open(p).convert('RGB') for p in source_images[:50]]

        print(f"  âœ“ ç”Ÿæˆåœ–åƒ: {len(gen_imgs)} å¼µ")
        print(f"  âœ“ åŸå§‹åœ–åƒ: {len(src_imgs)} å¼µ")

        report = {
            'generated_count': len(gen_imgs),
            'source_count': len(src_imgs),
            'analyses': {}
        }

        # 1. ç¾å­¸è³ªé‡
        print(f"\nğŸ¨ åˆ†æç¾å­¸è³ªé‡...")
        aesthetic_scores = []
        for img in tqdm(gen_imgs[:10], desc="  MUSIQ"):
            result = self.analyze_aesthetic_quality(img)
            if 'aesthetic_score' in result:
                aesthetic_scores.append(result['aesthetic_score'])

        if aesthetic_scores:
            report['analyses']['aesthetic'] = {
                'mean_score': float(np.mean(aesthetic_scores)),
                'std_score': float(np.std(aesthetic_scores)),
                'scores': aesthetic_scores
            }

        # 2. å…‰ç…§å±¬æ€§
        print(f"\nğŸ’¡ åˆ†æå…‰ç…§å±¬æ€§...")
        gen_lighting = []
        src_lighting = []

        for img in tqdm(gen_imgs[:20], desc="  ç”Ÿæˆåœ–åƒ"):
            result = self.analyze_lighting_properties(img)
            gen_lighting.append(result)

        for img in tqdm(src_imgs[:20], desc="  åŸå§‹åœ–åƒ"):
            result = self.analyze_lighting_properties(img)
            src_lighting.append(result)

        # å°æ¯”å…‰ç…§
        gen_contrast = np.mean([r['contrast_std'] for r in gen_lighting])
        src_contrast = np.mean([r['contrast_std'] for r in src_lighting])

        gen_uniformity = np.mean([r['lighting_uniformity'] for r in gen_lighting])
        src_uniformity = np.mean([r['lighting_uniformity'] for r in src_lighting])

        gen_pixar_score = np.mean([r['pixar_lighting_score'] for r in gen_lighting])
        src_pixar_score = np.mean([r['pixar_lighting_score'] for r in src_lighting])

        report['analyses']['lighting'] = {
            'generated': {
                'contrast': float(gen_contrast),
                'uniformity': float(gen_uniformity),
                'pixar_score': float(gen_pixar_score)
            },
            'source': {
                'contrast': float(src_contrast),
                'uniformity': float(src_uniformity),
                'pixar_score': float(src_pixar_score)
            },
            'comparison': {
                'contrast_diff_percent': float((gen_contrast - src_contrast) / src_contrast * 100),
                'uniformity_diff_percent': float((gen_uniformity - src_uniformity) / src_uniformity * 100),
                'pixar_score_diff': float(gen_pixar_score - src_pixar_score)
            }
        }

        # 3. äººè‡‰ä¸€è‡´æ€§
        print(f"\nğŸ‘¤ åˆ†æäººè‡‰ä¸€è‡´æ€§...")
        face_result = self.analyze_face_consistency(gen_imgs[:30])
        report['analyses']['face_consistency'] = face_result

        # 4. FID
        print(f"\nğŸ“Š è¨ˆç®— FID...")
        fid_result = self.compute_fid(src_imgs[:50], gen_imgs[:50])
        report['analyses']['fid'] = fid_result

        # 5. èªç¾©ç›¸ä¼¼åº¦ï¼ˆæŠ½æ¨£å°æ¯”ï¼‰
        print(f"\nğŸ” è¨ˆç®—èªç¾©ç›¸ä¼¼åº¦...")
        clip_similarities = []
        for i in range(min(10, len(gen_imgs), len(src_imgs))):
            result = self.analyze_semantic_similarity(gen_imgs[i], src_imgs[i])
            clip_similarities.append(result['clip_similarity'])

        report['analyses']['semantic'] = {
            'mean_clip_similarity': float(np.mean(clip_similarities)),
            'similarities': clip_similarities
        }

        # 6. æ„ŸçŸ¥è·é›¢ï¼ˆLPIPSï¼‰
        print(f"\nğŸ‘ï¸  è¨ˆç®—æ„ŸçŸ¥è·é›¢...")
        lpips_distances = []
        for i in range(min(10, len(gen_imgs), len(src_imgs))):
            result = self.analyze_perceptual_distance(gen_imgs[i], src_imgs[i])
            if 'lpips_distance' in result:
                lpips_distances.append(result['lpips_distance'])

        if lpips_distances:
            report['analyses']['perceptual'] = {
                'mean_lpips_distance': float(np.mean(lpips_distances)),
                'distances': lpips_distances
            }

        # ç”Ÿæˆè¨ºæ–·
        report['diagnosis'] = self.generate_diagnosis(report)

        # ä¿å­˜å ±å‘Š
        if output_path:
            output_path.parent.mkdir(parents=True, exist_ok=True)
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(report, f, indent=2, ensure_ascii=False)
            print(f"\nâœ… å ±å‘Šå·²ä¿å­˜: {output_path}")

        # æ‰“å°æ‘˜è¦
        self.print_summary(report)

        return report

    def generate_diagnosis(self, report: Dict) -> Dict:
        """æ ¹æ“šåˆ†æçµæœç”Ÿæˆè¨ºæ–·å’Œå»ºè­°"""
        diagnosis = {
            'issues': [],
            'recommendations': [],
            'caption_improvements': []
        }

        # å…‰ç…§å•é¡Œ
        if 'lighting' in report['analyses']:
            lighting = report['analyses']['lighting']

            contrast_diff = lighting['comparison']['contrast_diff_percent']
            if contrast_diff > 20:
                diagnosis['issues'].append({
                    'category': 'lighting',
                    'severity': 'high',
                    'description': f'å°æ¯”åº¦éé«˜ï¼ˆæ¯”åŸç‰‡é«˜ {contrast_diff:.1f}%ï¼‰',
                    'metrics': {
                        'generated_contrast': lighting['generated']['contrast'],
                        'source_contrast': lighting['source']['contrast']
                    }
                })

                diagnosis['recommendations'].append({
                    'category': 'training',
                    'priority': 'high',
                    'action': 'é™ä½å­¸ç¿’ç‡æˆ–æ¸›å°‘è¨“ç·´ epochs'
                })

                diagnosis['caption_improvements'].append({
                    'field': 'lighting',
                    'current': 'soft natural lighting',
                    'suggested': 'pixar uniform lighting, even illumination, low contrast, subtle ambient lighting'
                })

            pixar_score_diff = lighting['comparison']['pixar_score_diff']
            if pixar_score_diff < -20:
                diagnosis['issues'].append({
                    'category': 'style',
                    'severity': 'high',
                    'description': f'Pixar é¢¨æ ¼åˆ†æ•¸ä½æ–¼åŸç‰‡ {abs(pixar_score_diff):.1f} åˆ†',
                    'metrics': {
                        'generated_pixar_score': lighting['generated']['pixar_score'],
                        'source_pixar_score': lighting['source']['pixar_score']
                    }
                })

        # äººè‡‰ä¸€è‡´æ€§å•é¡Œ
        if 'face_consistency' in report['analyses']:
            face = report['analyses']['face_consistency']

            if 'avg_face_similarity' in face and face['avg_face_similarity'] < 0.6:
                diagnosis['issues'].append({
                    'category': 'consistency',
                    'severity': 'high',
                    'description': f'äººè‡‰ä¸€è‡´æ€§ä¸è¶³ï¼ˆç›¸ä¼¼åº¦: {face["avg_face_similarity"]:.2f}ï¼‰',
                    'metrics': face
                })

                diagnosis['recommendations'].append({
                    'category': 'training',
                    'priority': 'high',
                    'action': 'å¢åŠ  text encoder å­¸ç¿’ç‡ï¼Œå¼·åŒ–è§’è‰²ç‰¹å¾µå­¸ç¿’'
                })

        # FID åˆ†æ•¸
        if 'fid' in report['analyses']:
            fid = report['analyses']['fid']

            if 'fid_score' in fid and fid['fid_score'] > 100:
                diagnosis['issues'].append({
                    'category': 'distribution',
                    'severity': 'medium',
                    'description': f'ç”Ÿæˆåˆ†ä½ˆèˆ‡çœŸå¯¦åˆ†ä½ˆå·®ç•°è¼ƒå¤§ï¼ˆFID: {fid["fid_score"]:.1f}ï¼‰',
                    'metrics': fid
                })

        # ç¸½çµå»ºè­°
        if len(diagnosis['issues']) > 0:
            diagnosis['summary'] = {
                'total_issues': len(diagnosis['issues']),
                'high_priority': len([i for i in diagnosis['issues'] if i['severity'] == 'high']),
                'recommended_actions': self._prioritize_actions(diagnosis)
            }

        return diagnosis

    def _prioritize_actions(self, diagnosis: Dict) -> List[str]:
        """å„ªå…ˆæ’åºæ”¹å–„è¡Œå‹•"""
        actions = []

        # æª¢æŸ¥æ˜¯å¦æœ‰å…‰ç…§å•é¡Œ
        lighting_issues = [i for i in diagnosis['issues'] if i['category'] == 'lighting']
        if lighting_issues:
            actions.append("1. ç«‹å³ä¿®æ­£ caption æ·»åŠ å…‰ç…§æè¿°")
            actions.append("2. ä½¿ç”¨ --short é¸é …æ¸›å°‘ token æ¶ˆè€—")

        # æª¢æŸ¥æ˜¯å¦æœ‰ä¸€è‡´æ€§å•é¡Œ
        consistency_issues = [i for i in diagnosis['issues'] if i['category'] == 'consistency']
        if consistency_issues:
            actions.append("3. æé«˜ text_encoder_lr è‡³ UNet LR çš„ 75-80%")
            actions.append("4. æª¢æŸ¥ caption æ˜¯å¦å……åˆ†æè¿°è§’è‰²ç‰¹å¾µ")

        # æª¢æŸ¥æ˜¯å¦æœ‰åˆ†ä½ˆå•é¡Œ
        dist_issues = [i for i in diagnosis['issues'] if i['category'] == 'distribution']
        if dist_issues:
            actions.append("5. å¢åŠ è¨“ç·´æ•¸æ“šå¤šæ¨£æ€§")
            actions.append("6. è€ƒæ…®é™ä½ network_dim ä»¥æ¸›å°‘éæ“¬åˆ")

        return actions

    def print_summary(self, report: Dict):
        """æ‰“å°åˆ†ææ‘˜è¦"""
        print("\n" + "="*70)
        print("ğŸ“Š SOTA è³ªé‡åˆ†æå ±å‘Š")
        print("="*70)

        if 'aesthetic' in report['analyses']:
            aesthetic = report['analyses']['aesthetic']
            print(f"\nğŸ¨ ç¾å­¸è³ªé‡:")
            print(f"  å¹³å‡åˆ†æ•¸: {aesthetic['mean_score']:.2f}/10")

        if 'lighting' in report['analyses']:
            lighting = report['analyses']['lighting']
            print(f"\nğŸ’¡ å…‰ç…§åˆ†æ:")
            print(f"  ç”Ÿæˆåœ–åƒå°æ¯”åº¦: {lighting['generated']['contrast']:.1f}")
            print(f"  åŸå§‹åœ–åƒå°æ¯”åº¦: {lighting['source']['contrast']:.1f}")
            print(f"  å·®ç•°: {lighting['comparison']['contrast_diff_percent']:+.1f}%")
            print(f"\n  ç”Ÿæˆåœ–åƒ Pixar åˆ†æ•¸: {lighting['generated']['pixar_score']:.1f}/100")
            print(f"  åŸå§‹åœ–åƒ Pixar åˆ†æ•¸: {lighting['source']['pixar_score']:.1f}/100")

        if 'face_consistency' in report['analyses']:
            face = report['analyses']['face_consistency']
            if 'avg_face_similarity' in face:
                print(f"\nğŸ‘¤ äººè‡‰ä¸€è‡´æ€§:")
                print(f"  å¹³å‡ç›¸ä¼¼åº¦: {face['avg_face_similarity']:.3f}")
                print(f"  æª¢æ¸¬ç‡: {face['face_detection_rate']*100:.1f}%")

        if 'fid' in report['analyses']:
            fid = report['analyses']['fid']
            if 'fid_score' in fid:
                print(f"\nğŸ“Š FID åˆ†æ•¸: {fid['fid_score']:.2f}")

        if 'diagnosis' in report:
            diagnosis = report['diagnosis']

            if diagnosis['issues']:
                print(f"\nâš ï¸  ç™¼ç¾ {len(diagnosis['issues'])} å€‹å•é¡Œ:")
                for issue in diagnosis['issues'][:5]:
                    print(f"  [{issue['severity'].upper()}] {issue['description']}")

            if 'recommended_actions' in diagnosis.get('summary', {}):
                print(f"\nğŸ’¡ æ”¹å–„å»ºè­°:")
                for action in diagnosis['summary']['recommended_actions']:
                    print(f"  {action}")

        print("\n" + "="*70)


def main():
    parser = argparse.ArgumentParser(
        description="SOTA è³ªé‡åˆ†æå™¨ï¼šä½¿ç”¨æœ€å…ˆé€²çš„ AI æ¨¡å‹åˆ†æåœ–åƒè³ªé‡"
    )
    parser.add_argument(
        "--generated",
        type=Path,
        required=True,
        help="ç”Ÿæˆåœ–åƒç›®éŒ„"
    )
    parser.add_argument(
        "--source",
        type=Path,
        required=True,
        help="åŸå§‹åœ–åƒç›®éŒ„"
    )
    parser.add_argument(
        "--output",
        type=Path,
        default=None,
        help="è¼¸å‡ºå ±å‘Šè·¯å¾‘"
    )
    parser.add_argument(
        "--device",
        type=str,
        default="cuda",
        help="è¨­å‚™ (cuda/cpu)"
    )

    args = parser.parse_args()

    # å‰µå»ºåˆ†æå™¨
    analyzer = SOTAQualityAnalyzer(device=args.device)

    # ç²å–åœ–åƒåˆ—è¡¨
    generated_images = sorted(list(args.generated.glob("*.png")))
    source_images = sorted(list(args.source.glob("*.png")))

    if not generated_images:
        print(f"éŒ¯èª¤ï¼šç”Ÿæˆåœ–åƒç›®éŒ„ç‚ºç©º: {args.generated}")
        return 1

    if not source_images:
        print(f"éŒ¯èª¤ï¼šåŸå§‹åœ–åƒç›®éŒ„ç‚ºç©º: {args.source}")
        return 1

    # åŸ·è¡Œåˆ†æ
    analyzer.comprehensive_analysis(
        generated_images=generated_images,
        source_images=source_images,
        output_path=args.output
    )

    return 0


if __name__ == "__main__":
    exit(main())
