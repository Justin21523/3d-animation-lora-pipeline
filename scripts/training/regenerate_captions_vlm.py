#!/usr/bin/env python3
"""
ä½¿ç”¨ VLM (Qwen2-VL / InternVL2) å…¨é¢é‡æ–°ç”Ÿæˆ captions

æ ¹æ“š SOTA åˆ†æçµæœï¼Œç”Ÿæˆå„ªåŒ–çš„ captionsï¼ŒåŒ…å«ï¼š
1. Pixar çµ±ä¸€å…‰ç…§æè¿°
2. è©³ç´°è§’è‰²ç‰¹å¾µ
3. ä½å°æ¯”åº¦å¼·èª¿
4. é›»å½±ç´šè‰²å½©èª¿æ€§
5. 3D æè³ªæè¿°

æ”¯æŒçš„ VLMï¼š
- Qwen2-VL (æ¨è–¦)
- InternVL2
- BLIP2 (å‚™é¸)
"""

import argparse
import json
from pathlib import Path
from typing import Dict, List, Optional
import torch
from PIL import Image
from tqdm import tqdm
import warnings
warnings.filterwarnings('ignore')


class VLMCaptionGenerator:
    """VLM Caption ç”Ÿæˆå™¨"""

    def __init__(
        self,
        model_name: str = "qwen2_vl",
        device: str = "cuda",
        character_profile: Dict = None
    ):
        self.model_name = model_name
        self.device = device
        self.character_profile = character_profile or {}
        self.model = None
        self.processor = None

        print(f"ğŸš€ åˆå§‹åŒ– {model_name} æ¨¡å‹...")
        self.load_model()

    def load_model(self):
        """åŠ è¼‰ VLM æ¨¡å‹"""

        if self.model_name == "qwen2_vl":
            self._load_qwen2_vl()
        elif self.model_name == "internvl2":
            self._load_internvl2()
        elif self.model_name == "blip2":
            self._load_blip2()
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ¨¡å‹: {self.model_name}")

    def _load_qwen2_vl(self):
        """åŠ è¼‰ Qwen2-VL"""
        try:
            from transformers import Qwen2VLForConditionalGeneration, AutoProcessor

            # ä½¿ç”¨æœ¬åœ° ai_warehouse ä¸­çš„æ¨¡å‹
            model_id = "/mnt/c/AI_LLM_projects/ai_warehouse/models/vlm/Qwen2-VL-7B-Instruct"
            print(f"  ğŸ“¦ åŠ è¼‰ Qwen2-VL (æœ¬åœ°): {model_id}")

            self.model = Qwen2VLForConditionalGeneration.from_pretrained(
                model_id,
                torch_dtype=torch.float16,
                device_map="auto"
            )
            self.processor = AutoProcessor.from_pretrained(model_id)

            print("    âœ“ Qwen2-VL å·²åŠ è¼‰")

        except Exception as e:
            print(f"    âš ï¸  Qwen2-VL åŠ è¼‰å¤±æ•—: {e}")
            print("    ğŸ’¡ å®‰è£: pip install transformers>=4.37.0")
            raise

    def _load_internvl2(self):
        """åŠ è¼‰ InternVL2"""
        try:
            from transformers import AutoModel, AutoTokenizer

            model_id = "OpenGVLab/InternVL2-8B"
            print(f"  ğŸ“¦ åŠ è¼‰ InternVL2: {model_id}")

            self.model = AutoModel.from_pretrained(
                model_id,
                torch_dtype=torch.float16,
                trust_remote_code=True,
                device_map="auto"
            )
            self.tokenizer = AutoTokenizer.from_pretrained(
                model_id,
                trust_remote_code=True
            )

            print("    âœ“ InternVL2 å·²åŠ è¼‰")

        except Exception as e:
            print(f"    âš ï¸  InternVL2 åŠ è¼‰å¤±æ•—: {e}")
            raise

    def _load_blip2(self):
        """åŠ è¼‰ BLIP2ï¼ˆå‚™é¸ï¼‰"""
        try:
            from transformers import Blip2Processor, Blip2ForConditionalGeneration

            model_id = "Salesforce/blip2-opt-2.7b"
            print(f"  ğŸ“¦ åŠ è¼‰ BLIP2: {model_id}")

            self.processor = Blip2Processor.from_pretrained(model_id)
            self.model = Blip2ForConditionalGeneration.from_pretrained(
                model_id,
                torch_dtype=torch.float16,
                device_map="auto"
            )

            print("    âœ“ BLIP2 å·²åŠ è¼‰")

        except Exception as e:
            print(f"    âš ï¸  BLIP2 åŠ è¼‰å¤±æ•—: {e}")
            raise

    def create_prompt(self, analysis_result: Optional[Dict] = None) -> str:
        """
        å‰µå»º VLM prompt

        æ ¹æ“šåˆ†æçµæœèª¿æ•´ prompt ä»¥ä¿®æ­£å•é¡Œ
        """

        # åŸºç¤ prompt æ¨¡æ¿ï¼ˆè©³ç´°æŒ‡å°ï¼Œç¢ºä¿å®Œæ•´æè¿°ï¼‰
        base_prompt = """Analyze this 3D animated character image from Pixar's animation style and generate a detailed caption.

CRITICAL REQUIREMENTS (PRIORITIZED BY IMPORTANCE):
1. FACIAL FEATURES (HIGHEST PRIORITY): Provide EXTREMELY DETAILED facial descriptions:
   - Face shape, eyes (size, color, shape), eyebrows (thickness, arch), nose (size, shape)
   - Mouth expression, skin tone and texture, age-related features

2. EXPRESSION DETAILS: Describe the character's emotional state and facial expression:
   - Emotional state, eye expression, eyebrow movement, mouth expression

3. POSE & ACTION: Describe body position, gesture, and movement:
   - Body pose, head position, arm/hand gesture, specific action, camera angle/view

4. STYLE & RENDERING: "pixar film quality, 3d animation, cinematic rendering, smooth shading, subsurface scattering (SSS)"

5. LIGHTING: "pixar uniform lighting, even illumination, low contrast, subtle ambient lighting"

6. MATERIALS: "physically based rendering (PBR), matte skin shader, realistic fabric materials"

7. COLOR: "film color grading, balanced saturation, warm/cool tones"

"""

        # å¦‚æœæœ‰è§’è‰² profileï¼Œæ·»åŠ è§’è‰²ç‰¹å®šä¿¡æ¯ï¼ˆMUST BE INCLUDED IN OUTPUTï¼‰
        if self.character_profile:
            base_prompt += f"\nğŸ”´ MANDATORY CHARACTER IDENTITY (MUST APPEAR IN CAPTION):\n"
            base_prompt += "The character in this image is:\n"
            if 'core_description' in self.character_profile:
                base_prompt += f"âš ï¸ YOU MUST INCLUDE THIS EXACT DESCRIPTION IN YOUR CAPTION:\n"
                base_prompt += f'"{self.character_profile["core_description"]}"\n\n'
            if 'name' in self.character_profile:
                base_prompt += f"- Character name: {self.character_profile['name']}\n"
            if 'full_name' in self.character_profile:
                base_prompt += f"- Full name: {self.character_profile['full_name']}\n"
            if 'film' in self.character_profile:
                base_prompt += f"- From movie: {self.character_profile['film']}\n"
            if 'age' in self.character_profile:
                base_prompt += f"- Age: {self.character_profile['age']}\n"
            if 'physical_traits' in self.character_profile:
                base_prompt += f"- Key physical traits: {self.character_profile['physical_traits']}\n"

        base_prompt += """
TASK: Complete the caption template below by filling in [BLANKS] based on what you see in the image.

FIXED TEMPLATE (FILL IN THE BLANKS):
"a 3d animated character, pixar uniform lighting, even illumination, Luca Paguro from Pixar Luca (2021), 12-year-old italian pre-teen boy, large round brown eyes, thick arched eyebrows, button red-tinted nose, rosy cheeks, soft oval face, short dark-brown wavy curls, [HAIR_DETAILS], [EXPRESSION], [POSE_ACTION], pixar film quality, smooth shading, subsurface scattering on skin, matte skin shader, [CLOTHING_DETAILS], cinematic rendering, physically based rendering."

FILL IN THESE BLANKS:
1. [HAIR_DETAILS]: Describe hair style/state (e.g., "front quiff visible", "curls slightly tousled", "neat side part")
2. [EXPRESSION]: Facial expression (e.g., "surprised expression with wide eyes and raised eyebrows", "worried look with furrowed brows", "happy smile")
3. [POSE_ACTION]: Body pose and action (e.g., "lying on stomach underwater", "standing barefoot against off-white background", "looking up with open mouth")
4. [CLOTHING_DETAILS]: What is he wearing? (e.g., "barefoot with green mermaid tail", "barefoot in casual clothing", "shirtless")

EXAMPLE 1 (63 words):
"a 3d animated character, pixar uniform lighting, even illumination, Luca Paguro from Pixar Luca (2021), 12-year-old italian pre-teen boy, large round brown eyes, thick arched eyebrows, button red-tinted nose, rosy cheeks, soft oval face, short dark-brown wavy curls, front quiff visible, surprised expression with wide eyes, lying on stomach underwater, pixar film quality, smooth shading, subsurface scattering on skin, matte skin shader, green mermaid tail, cinematic rendering, physically based rendering."

EXAMPLE 2 (61 words):
"a 3d animated character, pixar uniform lighting, even illumination, Luca Paguro from Pixar Luca (2021), 12-year-old italian pre-teen boy, large round brown eyes, thick arched eyebrows, button red-tinted nose, rosy cheeks, soft oval face, short dark-brown wavy curls, neat side part, worried look with furrowed brows, standing barefoot against off-white background, pixar film quality, smooth shading, subsurface scattering on skin, matte skin shader, barefoot in casual wear, cinematic rendering, physically based rendering."

RULES:
- Output ONLY the complete filled template (one sentence, ending with period)
- Keep each blank SHORT (3-8 words)
- Total: 60-75 words target (important info in first 50 words)
- DO NOT add extra sentences or information
- DO NOT repeat any part of the template
"""

        return base_prompt

    def generate_caption(
        self,
        image: Image.Image,
        analysis_result: Optional[Dict] = None
    ) -> str:
        """ç”Ÿæˆå–®å¼µåœ–åƒçš„ caption"""

        prompt = self.create_prompt(analysis_result)

        if self.model_name == "qwen2_vl":
            return self._generate_qwen2_vl(image, prompt)
        elif self.model_name == "internvl2":
            return self._generate_internvl2(image, prompt)
        elif self.model_name == "blip2":
            return self._generate_blip2(image, prompt)

    def _generate_qwen2_vl(self, image: Image.Image, prompt: str) -> str:
        """ä½¿ç”¨ Qwen2-VL ç”Ÿæˆ"""

        messages = [
            {
                "role": "user",
                "content": [
                    {"type": "image", "image": image},
                    {"type": "text", "text": prompt}
                ]
            }
        ]

        text = self.processor.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True
        )

        inputs = self.processor(
            text=[text],
            images=[image],
            return_tensors="pt"
        ).to(self.device)

        # è¨˜éŒ„è¼¸å…¥é•·åº¦ï¼Œä»¥ä¾¿åªè§£ç¢¼æ–°ç”Ÿæˆçš„ tokens
        input_length = inputs['input_ids'].shape[1]

        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=120,  # å¢åŠ åˆ° 120 tokens (ç´„ 80-90 words) - åŒ…å«å®Œæ•´é¢¨æ ¼/æè³ªæè¿°
                do_sample=False,
                min_new_tokens=80,   # æœ€å°‘ 80 tokens (ç´„ 60 words) - ç¢ºä¿åŒ…å«æ ¸å¿ƒè³‡è¨Š
                repetition_penalty=1.3  # é˜²æ­¢é‡è¤‡æ®µè½ï¼ˆ1.0=ç„¡æ‡²ç½°, 1.3=é©åº¦æ‡²ç½°é‡è¤‡ï¼‰
            )

        # åªè§£ç¢¼æ–°ç”Ÿæˆçš„ tokensï¼ˆä¸åŒ…æ‹¬è¼¸å…¥ promptï¼‰
        generated_tokens = outputs[:, input_length:]
        caption = self.processor.batch_decode(
            generated_tokens,
            skip_special_tokens=True
        )[0]

        return self._clean_caption(caption)

    def _generate_internvl2(self, image: Image.Image, prompt: str) -> str:
        """ä½¿ç”¨ InternVL2 ç”Ÿæˆ"""

        pixel_values = self.model.vision_model.preprocess(image).to(
            self.device,
            dtype=torch.float16
        )

        with torch.no_grad():
            response = self.model.chat(
                self.tokenizer,
                pixel_values,
                prompt,
                generation_config={
                    'max_new_tokens': 150,
                    'do_sample': False
                }
            )

        return self._clean_caption(response)

    def _generate_blip2(self, image: Image.Image, prompt: str) -> str:
        """ä½¿ç”¨ BLIP2 ç”Ÿæˆ"""

        inputs = self.processor(image, text=prompt, return_tensors="pt").to(
            self.device,
            torch.float16
        )

        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=150,
                min_new_tokens=50
            )

        caption = self.processor.decode(outputs[0], skip_special_tokens=True)

        return self._clean_caption(caption)

    def _clean_caption(self, caption: str) -> str:
        """æ¸…ç†å’Œè¦ç¯„åŒ– caption"""
        import re

        # ç§»é™¤å¯èƒ½çš„å°è©±æ¨™è¨˜
        caption = caption.replace("Assistant:", "").replace("AI:", "")
        caption = caption.strip()

        # ç§»é™¤é‡è¤‡çš„å‰ç¶´ï¼ˆVLM æœ‰æ™‚æœƒé‡è¤‡ "a 3d animated character"ï¼‰
        prefix_pattern = r'^["\']?\s*a 3d animated character,?\s*'
        # ç§»é™¤æ‰€æœ‰å‰ç¶´å‡ºç¾
        while re.search(prefix_pattern, caption, flags=re.IGNORECASE):
            caption = re.sub(prefix_pattern, '', caption, count=1, flags=re.IGNORECASE).strip()

        # ç§»é™¤å¼•è™Ÿ
        caption = caption.replace('"', '').replace("'", '')

        # æ·»åŠ çµ±ä¸€å‰ç¶´ï¼ˆç¢ºä¿ä¸€è‡´æ€§ï¼‰
        caption = 'a 3d animated character, ' + caption

        # ç¢ºä¿çµå°¾æœ‰å¥è™Ÿ
        if caption and caption[-1] not in '.!?':
            caption += '.'

        return caption

    def batch_generate(
        self,
        image_dir: Path,
        output_dir: Path,
        analysis_result: Optional[Dict] = None,
        sample_size: Optional[int] = None
    ) -> Dict:
        """æ‰¹é‡ç”Ÿæˆ captions"""

        print(f"\nğŸ“¸ é–‹å§‹æ‰¹é‡ç”Ÿæˆ captions...")
        print(f"  è¼¸å…¥ç›®éŒ„: {image_dir}")
        print(f"  è¼¸å‡ºç›®éŒ„: {output_dir}")

        output_dir.mkdir(parents=True, exist_ok=True)

        # ç²å–æ‰€æœ‰åœ–åƒ
        image_files = sorted(list(image_dir.glob("*.png")))

        if sample_size:
            image_files = image_files[:sample_size]

        print(f"  æ‰¾åˆ° {len(image_files)} å¼µåœ–åƒ")

        results = {
            'total': len(image_files),
            'success': 0,
            'failed': 0,
            'captions': []
        }

        skipped = 0
        for img_path in tqdm(image_files, desc="  ç”Ÿæˆä¸­"):
            try:
                # æª¢æŸ¥æ˜¯å¦å·²æœ‰ captionï¼ˆè·³éå·²å­˜åœ¨çš„ï¼‰
                txt_path = output_dir / f"{img_path.stem}.txt"
                if txt_path.exists():
                    skipped += 1
                    continue

                # åŠ è¼‰åœ–åƒ
                image = Image.open(img_path).convert('RGB')

                # ç”Ÿæˆ caption
                caption = self.generate_caption(image, analysis_result)

                # ä¿å­˜ caption
                with open(txt_path, 'w', encoding='utf-8') as f:
                    f.write(caption)

                results['success'] += 1
                results['captions'].append({
                    'image': img_path.name,
                    'caption': caption
                })

            except Exception as e:
                print(f"\n  âš ï¸  è™•ç†å¤±æ•—: {img_path.name} - {e}")
                results['failed'] += 1

        print(f"\nâœ… å®Œæˆï¼")
        print(f"  æˆåŠŸ: {results['success']}")
        print(f"  è·³é: {skipped} (å·²æœ‰ caption)")
        print(f"  å¤±æ•—: {results['failed']}")

        # ä¿å­˜å…ƒæ•¸æ“š
        metadata_path = output_dir / "caption_metadata.json"
        with open(metadata_path, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)

        return results


def load_character_profile(profile_path: Path) -> Dict:
    """åŠ è¼‰è§’è‰² profile"""
    if profile_path and profile_path.exists():
        import yaml
        with open(profile_path, 'r', encoding='utf-8') as f:
            if profile_path.suffix in ['.yaml', '.yml']:
                return yaml.safe_load(f)
            else:
                return json.load(f)
    return {}


def load_analysis_result(analysis_path: Path) -> Optional[Dict]:
    """åŠ è¼‰ SOTA åˆ†æçµæœ"""
    if analysis_path and analysis_path.exists():
        with open(analysis_path, 'r', encoding='utf-8') as f:
            return json.load(f)
    return None


def main():
    parser = argparse.ArgumentParser(
        description="ä½¿ç”¨ VLM å…¨é¢é‡æ–°ç”Ÿæˆå„ªåŒ–çš„ captions"
    )
    parser.add_argument(
        "--image-dir",
        type=Path,
        required=True,
        help="åœ–åƒç›®éŒ„"
    )
    parser.add_argument(
        "--output-dir",
        type=Path,
        default=None,
        help="è¼¸å‡ºç›®éŒ„ï¼ˆé è¨­ï¼šè¦†è“‹åŸå§‹ captionï¼‰"
    )
    parser.add_argument(
        "--model",
        type=str,
        choices=["qwen2_vl", "internvl2", "blip2"],
        default="qwen2_vl",
        help="VLM æ¨¡å‹é¸æ“‡"
    )
    parser.add_argument(
        "--character-profile",
        type=Path,
        default=None,
        help="è§’è‰² profile JSON æ–‡ä»¶"
    )
    parser.add_argument(
        "--analysis-result",
        type=Path,
        default=None,
        help="SOTA åˆ†æçµæœ JSON æ–‡ä»¶ï¼ˆç”¨æ–¼é‡å°æ€§å„ªåŒ–ï¼‰"
    )
    parser.add_argument(
        "--sample-size",
        type=int,
        default=None,
        help="åƒ…è™•ç†å‰ N å¼µåœ–åƒï¼ˆæ¸¬è©¦ç”¨ï¼‰"
    )
    parser.add_argument(
        "--device",
        type=str,
        default="cuda",
        help="è¨­å‚™ (cuda/cpu)"
    )

    args = parser.parse_args()

    if not args.image_dir.exists():
        print(f"éŒ¯èª¤ï¼šåœ–åƒç›®éŒ„ä¸å­˜åœ¨: {args.image_dir}")
        return 1

    # è¼¸å‡ºç›®éŒ„
    output_dir = args.output_dir or args.image_dir

    # åŠ è¼‰è§’è‰² profile
    character_profile = load_character_profile(args.character_profile)

    # åŠ è¼‰åˆ†æçµæœ
    analysis_result = load_analysis_result(args.analysis_result)

    if analysis_result:
        print(f"âœ“ å·²åŠ è¼‰ SOTA åˆ†æçµæœï¼Œå°‡é‡å°æ€§å„ªåŒ– captions")

    # å‰µå»ºç”Ÿæˆå™¨
    generator = VLMCaptionGenerator(
        model_name=args.model,
        device=args.device,
        character_profile=character_profile
    )

    # æ‰¹é‡ç”Ÿæˆ
    results = generator.batch_generate(
        image_dir=args.image_dir,
        output_dir=output_dir,
        analysis_result=analysis_result,
        sample_size=args.sample_size
    )

    # é¡¯ç¤ºæ¨£æœ¬
    print(f"\nğŸ“ Caption æ¨£æœ¬ï¼ˆå‰ 3 å€‹ï¼‰ï¼š")
    for i, item in enumerate(results['captions'][:3], 1):
        print(f"\n{i}. {item['image']}")
        print(f"   {item['caption'][:100]}...")

    return 0


if __name__ == "__main__":
    exit(main())
