# å¹³è¡¡è¨“ç·´é…ç½®æŒ‡å—

**Balanced Training Configuration Guide**

Created: 2025-11-15
Version: 1.0.0

---

## ğŸ“‹ æ¦‚è¿°

æœ¬æŒ‡å—èªªæ˜å¦‚ä½•å¹³è¡¡ **ç©©å®šæ€§**ã€**é€Ÿåº¦** å’Œ **è¨˜æ†¶é«”ä½¿ç”¨**ï¼Œç¢ºä¿è¨“ç·´å¯ä»¥é•·æ™‚é–“é‹è¡Œè€Œä¸æœƒå´©æ½°æˆ–è€—ç›¡è³‡æºã€‚

---

## ğŸ¯ é…ç½®ç›®æ¨™

### å¹³è¡¡åŸå‰‡

1. **è¨˜æ†¶é«”ä½¿ç”¨ç›®æ¨™**: 60-70% RAMï¼Œ95% VRAM
2. **è¨“ç·´é€Ÿåº¦**: ä¸è¦å¤ªæ…¢ï¼ˆé¿å…æµªè²»æ™‚é–“ï¼‰
3. **ç©©å®šæ€§**: å¯é€£çºŒé‹è¡Œ 8-12 å°æ™‚ä¸å´©æ½°
4. **æ¢å¾©èƒ½åŠ›**: æ¯ 2 epochs ä¿å­˜ checkpoint

### å¯¦æ¸¬çµæœï¼ˆ30GB RAMï¼Œ16GB VRAM ç³»çµ±ï¼‰

| æŒ‡æ¨™ | ç›®æ¨™ç¯„åœ | å¯¦éš›è¡¨ç¾ |
|------|---------|---------|
| RAM ä½¿ç”¨ | 60-70% | 40% (12GB/30GB) âœ… |
| Available RAM | >8GB | 17GB âœ… |
| VRAM ä½¿ç”¨ | 90-98% | 97% (15.8GB/16.3GB) âœ… |
| GPU åˆ©ç”¨ç‡ | >95% | 99% âœ… |
| GPU æº«åº¦ | <80Â°C | 47Â°C âœ… |
| Swap ä½¿ç”¨ | <1GB | 283MB âœ… |

**çµè«–**: ç•¶å‰é…ç½®éå¸¸å¥åº·ï¼Œè¨˜æ†¶é«”æœ‰å……è¶³é¤˜è£•ï¼Œä¸æœƒéè¼‰ä¹Ÿä¸æœƒæµªè²»è³‡æºã€‚

---

## âš™ï¸ é—œéµé…ç½®åƒæ•¸

### 1. Batch èˆ‡ Accumulation

```toml
train_batch_size = 1
gradient_accumulation_steps = 6  # â­ å¹³è¡¡å€¼
```

**è§£é‡‹**:
- `train_batch_size = 1`: å¿…é ˆç‚º 1ï¼ˆ16GB VRAM é™åˆ¶ï¼‰
- `gradient_accumulation_steps`:
  - **4** = å¤ªä¿å®ˆï¼Œè¨“ç·´å¾ˆæ…¢
  - **6** = å¹³è¡¡ï¼Œé€Ÿåº¦èˆ‡ç©©å®šæ€§å…¼é¡§ âœ…
  - **8** = è¼ƒå¿«ï¼Œä½†è¨˜æ†¶é«”å£“åŠ›å¤§ï¼Œé•·æ™‚é–“é‹è¡Œå¯èƒ½å‡ºéŒ¯

**æœ‰æ•ˆ batch size** = 1 Ã— 6 = 6ï¼ˆè¶³å¤ ç©©å®šè¨“ç·´ï¼‰

---

### 2. Gradient Checkpointing

```toml
gradient_checkpointing = false  # â­ ç¦ç”¨
```

**åŸå› **:
- åœ¨ WSL2 + CUDA ç’°å¢ƒä¸‹ï¼Œé•·æ™‚é–“é‹è¡Œï¼ˆ>6 å°æ™‚ï¼‰æœƒå°è‡´ `CUDA unknown error`
- ç¦ç”¨å¾ŒçŠ§ç‰²å°‘é‡ VRAMï¼ˆ~2GBï¼‰ï¼Œæ›å–ç©©å®šæ€§

**å¦‚æœä½ æœ‰æ›´å¤š VRAMï¼ˆ24GB+ï¼‰**: å¯ä»¥å•Ÿç”¨ä»¥é€²ä¸€æ­¥é™ä½è¨˜æ†¶é«”ä½¿ç”¨

---

### 3. VAE Batch Size

```toml
vae_batch_size = 2  # â­ å¹³è¡¡å€¼
```

**è§£é‡‹**:
- **1** = å¤ªæ…¢ï¼ŒVAE ç·¨ç¢¼æ˜¯ç“¶é ¸
- **2** = å¹³è¡¡ï¼Œå¿« 2 å€ä¸”å®‰å…¨ âœ…
- **4+** = å¯èƒ½å°è‡´ VRAM OOM

---

### 4. Data Loader Workers

```toml
persistent_data_loader_workers = true
max_data_loader_n_workers = 2  # â­ å¹³è¡¡å€¼
```

**è§£é‡‹**:
- `persistent_data_loader_workers = true`: ä¿æŒ workers åœ¨è¨˜æ†¶é«”ï¼ˆåŠ é€Ÿï¼‰
- `max_data_loader_n_workers`:
  - **1** = å¤ªæ…¢ï¼Œæ•¸æ“šè¼‰å…¥æ˜¯ç“¶é ¸
  - **2** = å¹³è¡¡ï¼Œå……åˆ†åˆ©ç”¨ CPU âœ…
  - **4+** = RAM æ¶ˆè€—å¢åŠ ï¼Œä½†é€Ÿåº¦æå‡æœ‰é™

**è¨˜æ†¶é«”å½±éŸ¿**: æ¯å€‹ worker ~500MBï¼Œ2 å€‹ workers = 1GBï¼ˆå¯æ¥å—ï¼‰

---

### 5. Low RAM Mode

```toml
lowram = false  # â­ ç¦ç”¨ï¼ˆæˆ‘å€‘æœ‰ 30GB RAMï¼‰
```

**ä½•æ™‚å•Ÿç”¨**:
- ç³»çµ± RAM < 24GB
- Available RAM < 8GB
- é–‹å§‹ä½¿ç”¨ swap (>500MB)

**æˆ‘å€‘çš„æƒ…æ³**: æœ‰ 17GB å¯ç”¨ RAMï¼Œä¸éœ€è¦ `lowram` æ¨¡å¼

---

### 6. Latents Caching

```toml
cache_latents = true
cache_latents_to_disk = false
```

**è§£é‡‹**:
- `cache_latents = true`: é å…ˆç·¨ç¢¼æ‰€æœ‰åœ–ç‰‡ï¼ˆå¤§å¹…åŠ é€Ÿï¼‰
- `cache_latents_to_disk = false`: ä¿å­˜åœ¨ RAM è€Œéç¡¬ç¢Ÿ
  - éœ€è¦é¡å¤– ~3-5GB RAM
  - æˆ‘å€‘æœ‰ 17GB å¯ç”¨ï¼Œå®Œå…¨è¶³å¤  âœ…

**å¦‚æœ RAM ä¸è¶³**: æ”¹ç‚º `cache_latents_to_disk = true`ï¼ˆæœƒè®Šæ…¢ï¼‰

---

### 7. Training Duration

```toml
max_train_epochs = 12
save_every_n_epochs = 2
save_last_n_epochs = 3
```

**è§£é‡‹**:
- `max_train_epochs = 12`: ç¸®çŸ­è¨“ç·´é¿å…é•·æ™‚é–“é‹è¡Œå•é¡Œ
  - åŸæœ¬ 20 epochs åœ¨ epoch 4 (6.5 å°æ™‚) å°±å´©æ½°
  - 12 epochs é è¨ˆ 8-10 å°æ™‚å¯å®Œæˆ
- `save_every_n_epochs = 2`: é »ç¹ä¿å­˜ï¼Œæ–¹ä¾¿æ¢å¾©
- `save_last_n_epochs = 3`: ä¿ç•™æœ€å¾Œ 3 å€‹ checkpointsï¼ˆ2.6GB ç£ç¢Ÿç©ºé–“ï¼‰

---

## ğŸ“Š é…ç½®å°æ¯”è¡¨

| é…ç½®é …ç›® | ä¿å®ˆç‰ˆ | **å¹³è¡¡ç‰ˆ** âœ… | æ¿€é€²ç‰ˆ |
|---------|-------|------------|--------|
| `gradient_accumulation_steps` | 4 | **6** | 8 |
| `vae_batch_size` | 1 | **2** | 4 |
| `max_data_loader_n_workers` | 1 | **2** | 4 |
| `persistent_data_loader_workers` | false | **true** | true |
| `lowram` | true | **false** | false |
| `gradient_checkpointing` | false | **false** | true |
| **é æœŸ RAM ä½¿ç”¨** | 8-10GB | **12-14GB** | 16-20GB |
| **é æœŸ VRAM ä½¿ç”¨** | 12-13GB | **15-16GB** | 16GB+ (OOM) |
| **è¨“ç·´é€Ÿåº¦** | æ…¢ (-30%) | **æ¨™æº–** | å¿« (+15%) |
| **ç©©å®šæ€§** | éå¸¸é«˜ | **é«˜** | ä¸­ |

**æ¨è–¦**: å¹³è¡¡ç‰ˆï¼ˆç•¶å‰é…ç½®ï¼‰

---

## ğŸ” è¨˜æ†¶é«”ç›£æ§æŒ‡æ¨™

### å®‰å…¨ç¯„åœ

| æŒ‡æ¨™ | å®‰å…¨ç¯„åœ | è­¦å‘Šé–¾å€¼ | å±éšªé–¾å€¼ |
|------|---------|---------|---------|
| RAM ä½¿ç”¨ç‡ | <70% | 70-85% | >85% |
| Available RAM | >8GB | 4-8GB | <4GB |
| VRAM ä½¿ç”¨ç‡ | 90-98% | 85-90% æˆ– >98% | OOM éŒ¯èª¤ |
| Swap ä½¿ç”¨ | <500MB | 500MB-2GB | >2GB |
| GPU æº«åº¦ | <75Â°C | 75-85Â°C | >85Â°C |

### ç•¶å‰ç‹€æ…‹ âœ…

- RAM: 40% (12GB/30GB) - **éå¸¸å¥åº·**
- Available: 17GB - **å……è¶³é¤˜è£•**
- VRAM: 97% (15.8GB/16.3GB) - **æœ€ä½³ä½¿ç”¨**
- Swap: 283MB - **æ­£å¸¸**
- GPU Temp: 47Â°C - **å†·å»è‰¯å¥½**

---

## âš ï¸ å¸¸è¦‹å•é¡Œèˆ‡èª¿æ•´

### å•é¡Œ 1: RAM ä½¿ç”¨è¶…é 80%

**ç—‡ç‹€**: `free -h` é¡¯ç¤º available < 6GB

**è§£æ±ºæ–¹æ¡ˆ**:
```toml
# æ–¹æ¡ˆ A: é™ä½ workers
max_data_loader_n_workers = 1
persistent_data_loader_workers = false

# æ–¹æ¡ˆ B: å•Ÿç”¨ lowram
lowram = true

# æ–¹æ¡ˆ C: Latents å­˜ç¡¬ç¢Ÿ
cache_latents_to_disk = true
```

### å•é¡Œ 2: VRAM OOM éŒ¯èª¤

**ç—‡ç‹€**: `RuntimeError: CUDA out of memory`

**è§£æ±ºæ–¹æ¡ˆ**:
```toml
# æ–¹æ¡ˆ A: é™ä½ VAE batch
vae_batch_size = 1

# æ–¹æ¡ˆ B: é™ä½ accumulation
gradient_accumulation_steps = 4

# æ–¹æ¡ˆ C: å•Ÿç”¨ gradient checkpointingï¼ˆæœ‰é¢¨éšªï¼‰
gradient_checkpointing = true
```

### å•é¡Œ 3: è¨“ç·´é€Ÿåº¦å¤ªæ…¢

**ç—‡ç‹€**: æ¯å€‹ step è¶…é 3 ç§’

**å¯èƒ½åŸå› èˆ‡è§£æ±º**:
1. **workers å¤ªå°‘**:
   ```toml
   max_data_loader_n_workers = 2  # å¢åŠ åˆ° 2
   persistent_data_loader_workers = true
   ```

2. **VAE batch å¤ªå°**:
   ```toml
   vae_batch_size = 2  # å¾ 1 å¢åŠ åˆ° 2
   ```

3. **lowram æ¨¡å¼æ‹–æ…¢é€Ÿåº¦**:
   ```toml
   lowram = false  # å¦‚æœ RAM > 20GB
   ```

### å•é¡Œ 4: è¨“ç·´å¡ä½æˆ–æ›èµ·

**ç—‡ç‹€**: GPU åˆ©ç”¨ç‡çªç„¶é™åˆ° 0-5%ï¼Œè¶…é 30 åˆ†é˜ç„¡æ–° checkpoint

**æª¢æŸ¥**:
```bash
# æŸ¥çœ‹æœ€æ–° checkpoint æ™‚é–“
stat /mnt/data/ai_data/models/lora/luca/sdxl_trial1/*.safetensors

# æŸ¥çœ‹è¨“ç·´è¼¸å‡º
tmux attach -t sdxl_luca_training_safe
```

**è§£æ±º**: æ‰‹å‹•é‡å•Ÿæˆ–ç­‰å¾…å¥åº·ç›£æ§è‡ªå‹•é‡å•Ÿ

---

## ğŸš€ æ•ˆèƒ½å„ªåŒ–å»ºè­°

### å¦‚æœä½ æœ‰æ›´å¤šè³‡æº

**å¦‚æœ RAM > 48GB**:
```toml
max_data_loader_n_workers = 4
vae_batch_size = 4
```

**å¦‚æœ VRAM > 20GB**:
```toml
gradient_checkpointing = true  # å¯ä»¥å•Ÿç”¨
train_batch_size = 2           # å¯ä»¥å¢åŠ 
gradient_accumulation_steps = 4  # ç›¸æ‡‰é™ä½
```

### å¦‚æœè³‡æºæ›´å°‘

**å¦‚æœ RAM < 24GB**:
```toml
lowram = true
max_data_loader_n_workers = 1
persistent_data_loader_workers = false
cache_latents_to_disk = true
```

**å¦‚æœ VRAM < 12GB**:
ç„¡æ³•è¨“ç·´ SDXLï¼Œè«‹æ”¹ç”¨ SD 1.5

---

## ğŸ“ˆ é æœŸè¨“ç·´æ™‚é•·

### ä»¥ Luca æ•¸æ“šé›†ç‚ºä¾‹

**æ•¸æ“šé›†å¤§å°**: ~400 åœ–ç‰‡
**Epochs**: 12
**Total steps**: ~10,260 steps

**å¹³è¡¡é…ç½®é æœŸæ™‚é•·**:
- æ¯ step: ~2.5 ç§’
- æ¯ epoch: ~35 åˆ†é˜
- **ç¸½è¨ˆ: 7-8 å°æ™‚**

**èˆ‡å…¶ä»–é…ç½®å°æ¯”**:
- ä¿å®ˆé…ç½® (accumulation=4): ~10 å°æ™‚
- æ¿€é€²é…ç½® (accumulation=8): ~6 å°æ™‚ï¼ˆä½†é¢¨éšªé«˜ï¼‰

---

## âœ… æœ€ä½³å¯¦è¸

### è¨“ç·´å‰

1. âœ… æª¢æŸ¥ available RAM > 10GB
2. âœ… ç¢ºèªæ²’æœ‰å…¶ä»– GPU é€²ç¨‹
3. âœ… æ¸…ç†èˆŠçš„ tmux sessions
4. âœ… å•Ÿå‹•å¥åº·ç›£æ§

### è¨“ç·´ä¸­

1. âœ… æ¯ 1-2 å°æ™‚æª¢æŸ¥ GPU/RAM ç‹€æ…‹
2. âœ… æ¯ 2 epochs ç¢ºèªæ–° checkpoint å·²ä¿å­˜
3. âœ… ç›£æ§ swap ä½¿ç”¨ï¼ˆæ‡‰ä¿æŒ <500MBï¼‰
4. âœ… å¦‚æœ RAM ä½¿ç”¨ >80%ï¼Œè€ƒæ…®é‡å•Ÿä¸¦é™ä½é…ç½®

### è¨“ç·´å¾Œ

1. âœ… æ¸¬è©¦æ‰€æœ‰ checkpointsï¼ˆå°¤å…¶æ˜¯æœ€å¾Œ 3 å€‹ï¼‰
2. âœ… ä¿ç•™æœ€ä½³ checkpointï¼Œåˆªé™¤å…¶ä»–
3. âœ… è¨˜éŒ„è¨“ç·´æ™‚é•·å’Œé…ç½®ä¾›æœªä¾†åƒè€ƒ

---

## ğŸ”„ å‹•æ…‹èª¿æ•´ç­–ç•¥

### éšæ®µ 1: ä¿å®ˆé–‹å§‹

é¦–æ¬¡è¨“ç·´ä½¿ç”¨ä¿å®ˆé…ç½®ï¼ˆç¢ºä¿æˆåŠŸï¼‰:
```toml
gradient_accumulation_steps = 4
vae_batch_size = 1
max_data_loader_n_workers = 1
```

### éšæ®µ 2: é€æ­¥æå‡

å¦‚æœè¨“ç·´é †åˆ©ï¼ˆ2-3 epochs ç„¡å•é¡Œï¼‰ï¼Œé€æ­¥èª¿é«˜:
```toml
gradient_accumulation_steps = 6  # +50%
vae_batch_size = 2               # +100%
max_data_loader_n_workers = 2    # +100%
```

### éšæ®µ 3: æ‰¾åˆ°æ¥µé™

ç¹¼çºŒå°å¹…æå‡ç›´åˆ°é‡åˆ°å•é¡Œï¼Œç„¶å¾Œå›é€€ä¸€æ­¥ã€‚

---

## ğŸ“ æ•…éšœæ’é™¤è³‡æº

**ç›£æ§è…³æœ¬**:
```bash
# å¯¦æ™‚ç›£æ§ï¼ˆæ¨è–¦ï¼‰
bash /mnt/c/AI_LLM_projects/3d-animation-lora-pipeline/monitor_training_progress.sh

# å¿«é€Ÿç‹€æ…‹
bash /tmp/quick_status.sh

# GPU å¯¦æ™‚
watch -n 5 nvidia-smi
```

**å¥åº·ç›£æ§**:
```bash
# å•Ÿå‹•è‡ªå‹•ç›£æ§
bash scripts/monitoring/training_health_monitor.sh \
  --session sdxl_luca_training_safe \
  --output-dir /mnt/data/ai_data/models/lora/luca/sdxl_trial1 \
  --interval 300 \
  --max-restarts 3 &
```

**å®‰å…¨é‡å•Ÿ**:
```bash
# å¦‚æœè¨“ç·´å‡ºå•é¡Œ
bash scripts/training/safe_restart_training.sh
```

---

## ğŸ“ é…ç½®æª”æ¡ˆä½ç½®

**ç•¶å‰ä½¿ç”¨**: `/mnt/c/AI_LLM_projects/3d-animation-lora-pipeline/configs/training/sdxl_16gb_stable.toml`

**å®Œæ•´é…ç½®**: è¦‹ `configs/training/sdxl_16gb_stable.toml`

**ç›¸é—œæ–‡æª”**:
- `docs/guides/TRAINING_SAFETY_AND_RECOVERY.md` - å®‰å…¨æªæ–½èˆ‡æ¢å¾©
- `docs/guides/MONITORING_GUIDE.md` - ç›£æ§æŒ‡å—ï¼ˆæœ¬æ–‡æª”ï¼‰

---

**ç‰ˆæœ¬æ­·å²**:
- v1.0.0 (2025-11-15): åˆå§‹ç‰ˆæœ¬ï¼ŒåŸºæ–¼ 30GB RAM / 16GB VRAM ç³»çµ±å¯¦æ¸¬

**ä½œè€…**: Claude Code Assistant
**æ›´æ–°**: 2025-11-15
