# AIæ¨¡å‹ç®¡ç† - Warehouseçµ±ä¸€çµæ§‹

## ğŸ“‚ AI Warehouseç›®éŒ„çµæ§‹

æ‰€æœ‰æ¨¡å‹çµ±ä¸€å­˜æ”¾åœ¨ï¼š`/mnt/c/AI_LLM_projects/ai_warehouse/models/`

```
/mnt/c/AI_LLM_projects/ai_warehouse/models/
â”œâ”€â”€ base/                          # åŸºç¤ç”Ÿæˆæ¨¡å‹
â”‚   â”œâ”€â”€ stable-diffusion-v1-5/
â”‚   â”œâ”€â”€ stable-diffusion-v2-1/
â”‚   â””â”€â”€ sd-xl-base-1.0/
â”‚
â”œâ”€â”€ lora/                          # LoRAæ¨¡å‹è¼¸å‡º
â”‚   â”œâ”€â”€ luca/                      # æŒ‰é …ç›®çµ„ç¹”
â”‚   â”‚   â”œâ”€â”€ luca_human/
â”‚   â”‚   â”œâ”€â”€ alberto_human/
â”‚   â”‚   â””â”€â”€ iterative_overnight/  # è¿­ä»£è¨“ç·´è¼¸å‡º
â”‚   â”œâ”€â”€ toy_story/
â”‚   â””â”€â”€ [other_projects]/
â”‚
â”œâ”€â”€ vlm/                           # è¦–è¦ºèªè¨€æ¨¡å‹
â”‚   â”œâ”€â”€ Qwen2-VL-7B-Instruct/     # Captionç”Ÿæˆ
â”‚   â”œâ”€â”€ Qwen2-VL-2B-Instruct/
â”‚   â”œâ”€â”€ InternVL2-8B/              # è©•ä¼°ç”¨ï¼ˆSOTAï¼‰
â”‚   â””â”€â”€ BLIP2/
â”‚
â”œâ”€â”€ evaluation/                    # è©•ä¼°å°ˆç”¨æ¨¡å‹
â”‚   â”œâ”€â”€ clip/
â”‚   â”‚   â”œâ”€â”€ ViT-B-32/
â”‚   â”‚   â”œâ”€â”€ ViT-L-14/
â”‚   â”‚   â””â”€â”€ EVA-CLIP-L-14/         # SOTA CLIP
â”‚   â”œâ”€â”€ aesthetics/
â”‚   â”‚   â””â”€â”€ laion-aesthetics-v2/   # ç¾å­¸è©•åˆ†
â”‚   â”œâ”€â”€ face/
â”‚   â”‚   â”œâ”€â”€ arcface/
â”‚   â”‚   â””â”€â”€ insightface/           # è§’è‰²ä¸€è‡´æ€§
â”‚   â””â”€â”€ quality/
â”‚       â””â”€â”€ musiq/                 # åœ–åƒè³ªé‡
â”‚
â”œâ”€â”€ segmentation/                  # åˆ†å‰²æ¨¡å‹
â”‚   â”œâ”€â”€ sam2/
â”‚   â”‚   â”œâ”€â”€ sam2_hiera_large/
â”‚   â”‚   â””â”€â”€ sam2_hiera_base/
â”‚   â”œâ”€â”€ isnet/
â”‚   â””â”€â”€ u2net/
â”‚
â”œâ”€â”€ inpainting/                    # èƒŒæ™¯ä¿®å¾©
â”‚   â””â”€â”€ lama/
â”‚
â”œâ”€â”€ embedding/                     # ç‰¹å¾µæå–
â”‚   â”œâ”€â”€ openclip/
â”‚   â””â”€â”€ siglip/
â”‚
â””â”€â”€ utility/                       # è¼”åŠ©æ¨¡å‹
    â”œâ”€â”€ depth/
    â”‚   â”œâ”€â”€ zoedepth/
    â”‚   â””â”€â”€ midas/
    â”œâ”€â”€ pose/
    â”‚   â””â”€â”€ rtmpose/
    â””â”€â”€ face_detection/
        â””â”€â”€ retinaface/
```

---

## ğŸš€ æ¨¡å‹ä¸‹è¼‰è…³æœ¬

### å®Œæ•´ä¸‹è¼‰è…³æœ¬
```bash
#!/bin/bash
# ä¸‹è¼‰æ‰€æœ‰éœ€è¦çš„æ¨¡å‹åˆ°AI Warehouse

WAREHOUSE="/mnt/c/AI_LLM_projects/ai_warehouse/models"

echo "æ­£åœ¨ä¸‹è¼‰æ¨¡å‹åˆ° AI Warehouse..."

# ===== åŸºç¤ç”Ÿæˆæ¨¡å‹ =====
echo "1. ä¸‹è¼‰ Stable Diffusion åŸºç¤æ¨¡å‹..."
cd "$WAREHOUSE/base" || exit

# SD 1.5 (ä¸»è¦ä½¿ç”¨)
if [ ! -d "stable-diffusion-v1-5" ]; then
    git clone https://huggingface.co/runwayml/stable-diffusion-v1-5
fi

# ===== VLMæ¨¡å‹ =====
echo "2. ä¸‹è¼‰ VLMæ¨¡å‹ (Captionç”Ÿæˆ)..."
cd "$WAREHOUSE/vlm" || exit

# Qwen2-VL-7B (ä¸»è¦ä½¿ç”¨)
if [ ! -d "Qwen2-VL-7B-Instruct" ]; then
    huggingface-cli download Qwen/Qwen2-VL-7B-Instruct \
      --local-dir Qwen2-VL-7B-Instruct \
      --local-dir-use-symlinks False
fi

# InternVL2-8B (SOTAè©•ä¼°)
if [ ! -d "InternVL2-8B" ]; then
    huggingface-cli download OpenGVLab/InternVL2-8B \
      --local-dir InternVL2-8B \
      --local-dir-use-symlinks False
fi

# ===== è©•ä¼°æ¨¡å‹ =====
echo "3. ä¸‹è¼‰è©•ä¼°æ¨¡å‹..."
cd "$WAREHOUSE/evaluation" || exit

# CLIP (åŸºç¤è©•ä¼°)
mkdir -p clip
cd clip
python -c "import clip; clip.load('ViT-L/14')"  # æœƒè‡ªå‹•ä¸‹è¼‰åˆ°cache

# LAION Aesthetics (ç¾å­¸è©•åˆ†)
cd "$WAREHOUSE/evaluation/aesthetics"
huggingface-cli download cafeai/cafe_aesthetic \
  --local-dir laion-aesthetics-v2 \
  --local-dir-use-symlinks False

# InsightFace (è§’è‰²ä¸€è‡´æ€§)
cd "$WAREHOUSE/evaluation/face"
pip install insightface
python -c "import insightface; insightface.model_zoo.get_model('buffalo_l')"

# ===== åˆ†å‰²æ¨¡å‹ =====
echo "4. ä¸‹è¼‰åˆ†å‰²æ¨¡å‹..."
cd "$WAREHOUSE/segmentation" || exit

# SAM2
if [ ! -d "sam2" ]; then
    git clone https://github.com/facebookresearch/segment-anything-2.git sam2
    cd sam2
    wget https://dl.fbaipublicfiles.com/segment_anything_2/072824/sam2_hiera_large.pt
fi

# ISNet (via rembg)
pip install rembg

# ===== Inpaintingæ¨¡å‹ =====
echo "5. ä¸‹è¼‰ Inpaintingæ¨¡å‹..."
cd "$WAREHOUSE/inpainting" || exit

if [ ! -d "lama" ]; then
    git clone https://github.com/advimman/lama.git
    cd lama
    curl -LJO https://huggingface.co/smartywu/big-lama/resolve/main/big-lama.zip
    unzip big-lama.zip
fi

echo "âœ“ æ‰€æœ‰æ¨¡å‹ä¸‹è¼‰å®Œæˆï¼"
```

ä¿å­˜ç‚ºï¼š`scripts/setup/download_all_models.sh`

---

## ğŸ“ å„éšæ®µä½¿ç”¨çš„æ¨¡å‹

### Stage 1: Frame Extraction
**æ¨¡å‹éœ€æ±‚ï¼š** ç„¡ï¼ˆä½¿ç”¨OpenCV/ffmpegï¼‰

---

### Stage 2: Character Segmentation
**ä½ç½®ï¼š** `/mnt/c/AI_LLM_projects/ai_warehouse/models/segmentation/`

**ä½¿ç”¨æ¨¡å‹ï¼š**
- **ISNet** (é€šérembg) - ä¸»è¦ä½¿ç”¨
- **SAM2** - å¤šäººç‰©å ´æ™¯
- **UÂ²-Net** - å¿«é€Ÿé è¦½

**é…ç½®ç¤ºä¾‹ï¼š**
```python
# scripts/generic/segmentation/layered_segmentation.py

SEGMENTATION_MODELS = {
    'isnet': {
        'backend': 'rembg',
        'model': 'isnet-general-use',  # è‡ªå‹•ä¸‹è¼‰åˆ° ~/.u2net/
    },
    'sam2': {
        'checkpoint': '/mnt/c/AI_LLM_projects/ai_warehouse/models/segmentation/sam2/sam2_hiera_large.pt',
        'config': 'sam2_hiera_l.yaml'
    }
}
```

---

### Stage 3: Character Clustering
**ä½ç½®ï¼š** `/mnt/c/AI_LLM_projects/ai_warehouse/models/embedding/`

**ä½¿ç”¨æ¨¡å‹ï¼š**
- **CLIP ViT-L/14** - è¦–è¦ºembedding
- **ArcFace** - äººè‡‰è­˜åˆ¥
- **RTM-Pose** - å§¿æ…‹ä¼°è¨ˆï¼ˆå¯é¸ï¼‰

**é…ç½®ç¤ºä¾‹ï¼š**
```python
# scripts/generic/clustering/character_clustering.py

CLIP_MODEL = "ViT-L/14"  # æœƒè‡ªå‹•åŠ è¼‰åˆ° torch cache
# æˆ–æŒ‡å®šæœ¬åœ°è·¯å¾‘ï¼š
# CLIP_MODEL_PATH = "/mnt/c/AI_LLM_projects/ai_warehouse/models/evaluation/clip/ViT-L-14"

ARCFACE_MODEL = "/mnt/c/AI_LLM_projects/ai_warehouse/models/evaluation/face/arcface"
```

---

### Stage 4: Caption Generation
**ä½ç½®ï¼š** `/mnt/c/AI_LLM_projects/ai_warehouse/models/vlm/`

**ä½¿ç”¨æ¨¡å‹ï¼š**
- **Qwen2-VL-7B-Instruct** (ä¸»è¦)
- **InternVL2-8B** (å‚™é¸)

**é…ç½®ç¤ºä¾‹ï¼š**
```python
# scripts/generic/training/qwen_caption_generator.py

MODEL_PATH = "/mnt/c/AI_LLM_projects/ai_warehouse/models/vlm/Qwen2-VL-7B-Instruct"

self.model = Qwen2VLForConditionalGeneration.from_pretrained(
    MODEL_PATH,
    torch_dtype=torch.bfloat16,
    device_map="auto",
    local_files_only=True  # åªä½¿ç”¨æœ¬åœ°æ–‡ä»¶
)
```

---

### Stage 5: LoRA Training
**ä½ç½®ï¼š** `/mnt/c/AI_LLM_projects/ai_warehouse/models/base/`

**ä½¿ç”¨æ¨¡å‹ï¼š**
- **Stable Diffusion v1.5** (ä¸»è¦)

**é…ç½®ç¤ºä¾‹ï¼š**
```toml
# configs/projects/luca/luca_human.toml

pretrained_model_name_or_path = "/mnt/c/AI_LLM_projects/ai_warehouse/models/base/stable-diffusion-v1-5"
```

**è¼¸å‡ºä½ç½®ï¼š**
```
/mnt/c/AI_LLM_projects/ai_warehouse/models/lora/luca/
â”œâ”€â”€ luca_human/
â”‚   â”œâ”€â”€ luca_human_v1-000015.safetensors
â”‚   â””â”€â”€ ...
â””â”€â”€ iterative_overnight/
    â”œâ”€â”€ iteration_1/
    â”œâ”€â”€ iteration_2/
    â””â”€â”€ ...
```

---

### Stage 6: LoRA Evaluation
**ä½ç½®ï¼š** `/mnt/c/AI_LLM_projects/ai_warehouse/models/evaluation/`

**ä½¿ç”¨æ¨¡å‹ï¼š**
- **CLIP ViT-L/14** - CLIP Score
- **InternVL2-8B** (SOTAå‡ç´š)
- **InsightFace** - è§’è‰²ä¸€è‡´æ€§
- **LAION Aesthetics** - ç¾å­¸è©•åˆ†
- **MUSIQ** - åœ–åƒè³ªé‡

**é…ç½®ç¤ºä¾‹ï¼š**
```python
# scripts/evaluation/auto_lora_evaluator.py

EVALUATION_MODELS = {
    'clip': '/mnt/c/AI_LLM_projects/ai_warehouse/models/evaluation/clip/ViT-L-14',
    'internvl': '/mnt/c/AI_LLM_projects/ai_warehouse/models/vlm/InternVL2-8B',
    'aesthetics': '/mnt/c/AI_LLM_projects/ai_warehouse/models/evaluation/aesthetics/laion-aesthetics-v2',
    'insightface': '/mnt/c/AI_LLM_projects/ai_warehouse/models/evaluation/face/insightface',
}
```

---

## ğŸ”§ è·¯å¾‘é…ç½®ç®¡ç†

### å‰µå»ºå…¨å±€é…ç½®

**ä½ç½®ï¼š** `config/model_paths.yaml`

```yaml
# AI Warehouseæ¨¡å‹è·¯å¾‘é…ç½®
# æ‰€æœ‰è…³æœ¬çµ±ä¸€å¾é€™è£¡è®€å–è·¯å¾‘

warehouse_root: "/mnt/c/AI_LLM_projects/ai_warehouse/models"

base_models:
  sd_v1_5: "${warehouse_root}/base/stable-diffusion-v1-5"
  sd_v2_1: "${warehouse_root}/base/stable-diffusion-v2-1"
  sdxl: "${warehouse_root}/base/sd-xl-base-1.0"

vlm_models:
  qwen2_vl_7b: "${warehouse_root}/vlm/Qwen2-VL-7B-Instruct"
  qwen2_vl_2b: "${warehouse_root}/vlm/Qwen2-VL-2B-Instruct"
  internvl2_8b: "${warehouse_root}/vlm/InternVL2-8B"

evaluation_models:
  clip_vit_l: "${warehouse_root}/evaluation/clip/ViT-L-14"
  eva_clip: "${warehouse_root}/evaluation/clip/EVA-CLIP-L-14"
  aesthetics: "${warehouse_root}/evaluation/aesthetics/laion-aesthetics-v2"
  insightface: "${warehouse_root}/evaluation/face/insightface"
  musiq: "${warehouse_root}/evaluation/quality/musiq"

segmentation_models:
  sam2_large: "${warehouse_root}/segmentation/sam2/sam2_hiera_large.pt"
  sam2_base: "${warehouse_root}/segmentation/sam2/sam2_hiera_base.pt"
  u2net: "${warehouse_root}/segmentation/u2net"

inpainting_models:
  lama: "${warehouse_root}/inpainting/lama"

lora_output:
  base_dir: "${warehouse_root}/lora"
```

### Pythonè®€å–é…ç½®

```python
# scripts/core/utils/model_paths.py

import yaml
from pathlib import Path
from string import Template

def load_model_paths():
    """åŠ è¼‰æ¨¡å‹è·¯å¾‘é…ç½®"""
    config_path = Path(__file__).parent.parent.parent.parent / "config" / "model_paths.yaml"

    with open(config_path, 'r') as f:
        config = yaml.safe_load(f)

    # å±•é–‹è®Šé‡
    warehouse_root = config['warehouse_root']

    def expand_path(path_str):
        return Template(path_str).substitute(warehouse_root=warehouse_root)

    # éæ­¸å±•é–‹æ‰€æœ‰è·¯å¾‘
    def expand_dict(d):
        for key, value in d.items():
            if isinstance(value, str):
                d[key] = expand_path(value)
            elif isinstance(value, dict):
                expand_dict(value)

    expand_dict(config)

    return config

# ä½¿ç”¨ç¤ºä¾‹
MODEL_PATHS = load_model_paths()

# åœ¨è…³æœ¬ä¸­å¼•ç”¨
from scripts.core.utils.model_paths import MODEL_PATHS

vlm_model_path = MODEL_PATHS['vlm_models']['qwen2_vl_7b']
base_model_path = MODEL_PATHS['base_models']['sd_v1_5']
```

---

## ğŸ“¦ å¿«é€Ÿæª¢æŸ¥æ¨¡å‹

```bash
#!/bin/bash
# scripts/setup/verify_models.sh
# é©—è­‰æ‰€æœ‰å¿…éœ€æ¨¡å‹æ˜¯å¦å·²ä¸‹è¼‰

WAREHOUSE="/mnt/c/AI_LLM_projects/ai_warehouse/models"

echo "æª¢æŸ¥AI Warehouseæ¨¡å‹..."

check_model() {
    local path=$1
    local name=$2

    if [ -e "$path" ]; then
        echo "  âœ“ $name"
    else
        echo "  âœ— $name (MISSING)"
    fi
}

echo ""
echo "åŸºç¤æ¨¡å‹:"
check_model "$WAREHOUSE/base/stable-diffusion-v1-5" "SD v1.5"

echo ""
echo "VLMæ¨¡å‹:"
check_model "$WAREHOUSE/vlm/Qwen2-VL-7B-Instruct" "Qwen2-VL-7B"
check_model "$WAREHOUSE/vlm/InternVL2-8B" "InternVL2-8B (SOTA)"

echo ""
echo "è©•ä¼°æ¨¡å‹:"
check_model "$WAREHOUSE/evaluation/clip" "CLIP"
check_model "$WAREHOUSE/evaluation/aesthetics/laion-aesthetics-v2" "LAION Aesthetics"
check_model "$WAREHOUSE/evaluation/face/insightface" "InsightFace"

echo ""
echo "åˆ†å‰²æ¨¡å‹:"
check_model "$WAREHOUSE/segmentation/sam2" "SAM2"

echo ""
echo "Inpaintingæ¨¡å‹:"
check_model "$WAREHOUSE/inpainting/lama" "LaMa"

echo ""
echo "æª¢æŸ¥å®Œæˆï¼"
```

---

## ğŸ¯ é …ç›®ç‰¹å®šé…ç½®

### Lucaé …ç›®å¿«é€Ÿå•Ÿå‹•é…ç½®

**å‰µå»ºï¼š** `configs/projects/luca/model_config.yaml`

```yaml
project: luca
style: pixar_3d

base_model: "/mnt/c/AI_LLM_projects/ai_warehouse/models/base/stable-diffusion-v1-5"

caption_model: "/mnt/c/AI_LLM_projects/ai_warehouse/models/vlm/Qwen2-VL-7B-Instruct"

evaluation_models:
  clip: "ViT-L/14"  # æœƒä½¿ç”¨torch cache
  aesthetics: "/mnt/c/AI_LLM_projects/ai_warehouse/models/evaluation/aesthetics/laion-aesthetics-v2"
  insightface: "/mnt/c/AI_LLM_projects/ai_warehouse/models/evaluation/face/insightface"

segmentation_model: "isnet"  # ä½¿ç”¨rembg (è‡ªå‹•ç®¡ç†)

lora_output_dir: "/mnt/c/AI_LLM_projects/ai_warehouse/models/lora/luca"
```

---

## ğŸ’¾ ç£ç¢Ÿç©ºé–“ä¼°ç®—

| æ¨¡å‹é¡åˆ¥ | å¤§å° | å¿…éœ€ |
|---------|------|------|
| SD v1.5 | ~5GB | âœ… |
| Qwen2-VL-7B | ~15GB | âœ… |
| InternVL2-8B | ~16GB | âš ï¸ SOTAå‡ç´š |
| CLIP ViT-L/14 | ~1GB | âœ… |
| SAM2 Large | ~3GB | âš ï¸ å¤šäººå ´æ™¯ |
| InsightFace | ~500MB | âš ï¸ é«˜ç´šè©•ä¼° |
| LAION Aesthetics | ~300MB | âš ï¸ é«˜ç´šè©•ä¼° |
| LaMa Inpainting | ~200MB | âš ï¸ èƒŒæ™¯ä¿®å¾© |

**ç¸½è¨ˆï¼š**
- **åŸºç¤é…ç½® (å¿…éœ€):** ~22GB
- **å®Œæ•´é…ç½® (å«SOTA):** ~41GB

---

## ğŸš€ ä¸€éµè¨­ç½®

```bash
# 1. å‰µå»ºç›®éŒ„çµæ§‹
bash scripts/setup/create_warehouse_structure.sh

# 2. ä¸‹è¼‰åŸºç¤æ¨¡å‹
bash scripts/setup/download_all_models.sh

# 3. é©—è­‰å®‰è£
bash scripts/setup/verify_models.sh

# 4. é‹è¡Œæ¸¬è©¦
python scripts/setup/test_model_loading.py
```

---

## âœ… æœ€ä½³å¯¦è¸

1. âœ… **çµ±ä¸€è·¯å¾‘ç®¡ç†** - æ‰€æœ‰æ¨¡å‹åœ¨warehouseï¼Œé€šéé…ç½®æ–‡ä»¶å¼•ç”¨
2. âœ… **ç‰ˆæœ¬æ§åˆ¶** - ä½¿ç”¨ç¬¦è™Ÿéˆæ¥æŒ‡å‘ç‰¹å®šç‰ˆæœ¬
3. âœ… **å®šæœŸå‚™ä»½** - é‡è¦çš„LoRAè¼¸å‡ºå®šæœŸå‚™ä»½
4. âœ… **æ¸…ç†cache** - å®šæœŸæ¸…ç†HuggingFace cacheå’Œtorch cache
5. âœ… **æ–‡æª”è¨˜éŒ„** - æ¯å€‹æ¨¡å‹çš„ç”¨é€”å’Œç‰ˆæœ¬è¨˜éŒ„åœ¨warehouse

---

**ç‰ˆæœ¬ï¼š** v1.0
**æœ€å¾Œæ›´æ–°ï¼š** 2025-11-10
