# SDXL è¨“ç·´å¿«é€Ÿåƒè€ƒå¡

## ğŸ¯ æ ¸å¿ƒæ¦‚å¿µ

**å¯ä»¥é·ç§»**: âœ… è¶…åƒæ•¸ã€æ•¸æ“šé›†ã€è¨“ç·´ç­–ç•¥
**éœ€è¦èª¿æ•´**: âš ï¸ åˆ†è¾¨ç‡ã€batch sizeã€base model
**å®Œå…¨ä¸åŒ**: âŒ æ¨¡å‹æ¶æ§‹ã€VRAM éœ€æ±‚ã€è¨“ç·´æ™‚é–“

---

## ğŸ“Š SD1.5 vs SDXL å°æ¯”è¡¨

| é …ç›® | SD1.5 | SDXL |
|------|-------|------|
| **åˆ†è¾¨ç‡** | 512Ã—512 | 1024Ã—1024 |
| **æ¨¡å‹å¤§å°** | 0.9B | 2.6B |
| **Text Encoders** | 1 (CLIP-L) | 2 (CLIP-L + OpenCLIP-G) |
| **VRAM éœ€æ±‚** | 8-12 GB | 16-24 GB |
| **Batch Size** | 8-16 | 2-4 |
| **è¨“ç·´æ™‚é–“/epoch** | ~15 min | ~30-45 min |
| **LoRA å¤§å° (dim=64)** | ~73 MB | ~190 MB |
| **åœ–ç‰‡è³ªé‡** | 7/10 | 9/10 |

---

## âš¡ ä¸€éµå•Ÿå‹• SDXL è¨“ç·´

### å‰ç½®æ¢ä»¶
```bash
# 1. SD1.5 å„ªåŒ–å·²å®Œæˆï¼Œæå–æœ€ä½³åƒæ•¸
BEST_LR=0.0003
BEST_DIM=64
BEST_ALPHA=32
BEST_OPTIMIZER="AdamW8bit"
BEST_SCHEDULER="cosine_with_restarts"
BEST_GRAD_ACCUM=2
BEST_EPOCHS=12

# 2. ä¸‹è¼‰ SDXL base model
cd /mnt/c/AI_LLM_projects/ai_warehouse/models/stable-diffusion/sdxl
wget https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/resolve/main/sd_xl_base_1.0.safetensors
```

### è¨“ç·´å‘½ä»¤ï¼ˆè¤‡è£½å³ç”¨ï¼‰
```bash
cd /mnt/c/AI_LLM_projects/kohya_ss/sd-scripts

nohup conda run -n kohya_ss python train_network.py \
  --dataset_config /mnt/c/AI_LLM_projects/3d-animation-lora-pipeline/configs/training/sdxl/luca_human_dataset_sdxl.toml \
  --pretrained_model_name_or_path /mnt/c/AI_LLM_projects/ai_warehouse/models/stable-diffusion/sdxl/sd_xl_base_1.0.safetensors \
  --output_dir /mnt/data/ai_data/models/lora/luca/sdxl_v1 \
  --output_name luca_sdxl_v1 \
  --network_module networks.lora \
  --network_dim $BEST_DIM \
  --network_alpha $BEST_ALPHA \
  --learning_rate $BEST_LR \
  --text_encoder_lr $(echo "$BEST_LR * 0.67" | bc -l) \
  --unet_lr $BEST_LR \
  --max_train_epochs $BEST_EPOCHS \
  --save_every_n_epochs 2 \
  --save_model_as safetensors \
  --save_precision fp16 \
  --mixed_precision fp16 \
  --gradient_checkpointing \
  --gradient_accumulation_steps $BEST_GRAD_ACCUM \
  --optimizer_type $BEST_OPTIMIZER \
  --lr_scheduler $BEST_SCHEDULER \
  --lr_scheduler_num_cycles 3 \
  --lr_warmup_steps 100 \
  --logging_dir /mnt/data/ai_data/models/lora/luca/sdxl_v1/logs \
  --log_with tensorboard \
  --seed 42 \
  --clip_skip 2 \
  --cache_latents \
  --cache_latents_to_disk \
  --max_data_loader_n_workers 8 \
  --persistent_data_loader_workers \
  --xformers \
  --max_token_length 225 \
  --bucket_reso_steps 64 \
  --bucket_no_upscale \
  > /mnt/data/ai_data/models/lora/luca/sdxl_v1/training.log 2>&1 &

echo "SDXL è¨“ç·´å·²å•Ÿå‹•ï¼ŒPID: $!"
```

---

## ğŸ”§ é—œéµåƒæ•¸èªªæ˜

### SDXL ç‰¹æœ‰åƒæ•¸ï¼ˆå¿…é ˆæ·»åŠ ï¼‰
```bash
--text_encoder_lr 0.0002       # é›™ text encoderï¼Œç¨ç«‹è¨­ç½®
--unet_lr 0.0003               # U-Net å­¸ç¿’ç‡
--max_token_length 225         # SDXL æ”¯æŒæ›´é•· tokens
--xformers                     # è¨˜æ†¶é«”å„ªåŒ–ï¼ˆå¿…é ˆï¼‰
```

### å¾ SD1.5 é·ç§»çš„åƒæ•¸ï¼ˆç›´æ¥ä½¿ç”¨ï¼‰
```bash
--network_dim 64               # SD1.5 æœ€ä½³å€¼
--network_alpha 32             # SD1.5 æœ€ä½³å€¼
--learning_rate 0.0003         # SD1.5 æœ€ä½³å€¼
--optimizer_type AdamW8bit     # SD1.5 æœ€ä½³å€¼
--lr_scheduler cosine_with_restarts  # SD1.5 æœ€ä½³å€¼
--gradient_accumulation_steps 2      # SD1.5 æœ€ä½³å€¼
--max_train_epochs 12          # SD1.5 æœ€ä½³å€¼ï¼ˆæˆ– +20%ï¼‰
```

### éœ€è¦èª¿æ•´çš„åƒæ•¸
```bash
--batch_size 4                 # SD1.5: 8 â†’ SDXL: 4ï¼ˆVRAM é™åˆ¶ï¼‰
--text_encoder_lr              # æ–°å¢ï¼Œç´„ç‚º learning_rate Ã— 0.5-0.8
```

---

## ğŸ“ˆ ç›£æ§å‘½ä»¤

### å¯¦æ™‚æ—¥èªŒ
```bash
tail -f /mnt/data/ai_data/models/lora/luca/sdxl_v1/training.log
```

### æŸ¥çœ‹ Checkpoints
```bash
watch -n 60 'ls -lh /mnt/data/ai_data/models/lora/luca/sdxl_v1/*.safetensors'
```

### TensorBoard å¯è¦–åŒ–
```bash
tensorboard --logdir /mnt/data/ai_data/models/lora/luca/sdxl_v1/logs --port 6007 --bind_all
```

### è¨“ç·´é€²åº¦ä¼°ç®—
```bash
# æŸ¥çœ‹ç•¶å‰ epoch
grep -oP "Epoch \d+/\d+" /mnt/data/ai_data/models/lora/luca/sdxl_v1/training.log | tail -1

# æŸ¥çœ‹ loss è¶¨å‹¢
grep "loss:" /mnt/data/ai_data/models/lora/luca/sdxl_v1/training.log | tail -20
```

---

## ğŸ§ª è©•ä¼° SDXL Checkpoint

### å–®å€‹ Checkpoint æ¸¬è©¦
```bash
conda run -n kohya_ss python /mnt/c/AI_LLM_projects/3d-animation-lora-pipeline/scripts/evaluation/evaluate_single_checkpoint.py \
  --checkpoint /mnt/data/ai_data/models/lora/luca/sdxl_v1/luca_sdxl_v1-000008.safetensors \
  --base-model /mnt/c/AI_LLM_projects/ai_warehouse/models/stable-diffusion/sdxl/sd_xl_base_1.0.safetensors \
  --output-dir /mnt/data/ai_data/models/lora/luca/sdxl_v1/eval_epoch8 \
  --num-samples 16 \
  --device cuda \
  --resolution 1024
```

### å°æ¯” SD1.5 vs SDXL è³ªé‡
ç”Ÿæˆç›¸åŒ prompt çš„åœ–ç‰‡ä¸¦æ¯”è¼ƒï¼š
- **SD1.5**: 512Ã—512, è¼ƒå¿«, è³ªé‡ä¸­ç­‰
- **SDXL**: 1024Ã—1024, è¼ƒæ…¢, è³ªé‡å„ªç§€

---

## âš ï¸ å¸¸è¦‹å•é¡Œå¿«é€Ÿè§£æ±º

### âŒ OOM (Out of Memory)
```bash
# è§£æ±ºæ–¹æ¡ˆ 1ï¼šé™ä½ batch size
--batch_size 2
--gradient_accumulation_steps 4  # ç­‰æ•ˆ batch size = 8

# è§£æ±ºæ–¹æ¡ˆ 2ï¼šå•Ÿç”¨è¨˜æ†¶é«”å„ªåŒ–
--xformers  # å¿…é ˆ
--gradient_checkpointing  # å¿…é ˆ

# è§£æ±ºæ–¹æ¡ˆ 3ï¼šæ¥µç«¯æƒ…æ³
--lowvram
--medvram
```

### ğŸŒ è¨“ç·´å¤ªæ…¢
- **æ­£å¸¸ç¾è±¡**ï¼šSDXL é æœŸæ…¢ 2-3 å€
- **ç„¡æ³•åŠ é€Ÿ**ï¼šæ¨¡å‹å¤§å°å’Œåˆ†è¾¨ç‡æ±ºå®š
- **å»ºè­°**ï¼šä½¿ç”¨ overnight è¨“ç·´

### ğŸ–¼ï¸ åœ–ç‰‡è³ªé‡ä¸ä½³
- **æª¢æŸ¥**ï¼šæ˜¯å¦ä½¿ç”¨äº† SD1.5 æœ€ä½³è¶…åƒæ•¸ï¼Ÿ
- **æª¢æŸ¥**ï¼šdataset é…ç½®æ˜¯å¦æ­£ç¢ºï¼ˆresolution=1024ï¼‰ï¼Ÿ
- **æª¢æŸ¥**ï¼šæ˜¯å¦åœ¨æ­£ç¢ºçš„ epochï¼ˆé€šå¸¸ epoch 6-10 æœ€ä½³ï¼‰ï¼Ÿ

### ğŸ“¦ LoRA æª”æ¡ˆå¤ªå¤§
- **æ­£å¸¸**ï¼šSDXL LoRA ç´„ 2.5 å€ SD1.5 å¤§å°
- **å„ªåŒ–**ï¼šé™ä½ network_dimï¼ˆä½†å¯èƒ½å½±éŸ¿è³ªé‡ï¼‰
- **å»ºè­°**ï¼šä¿æŒ dim=64 æˆ– 128

---

## ğŸ“ æœ€ä½³å¯¦è¸ Checklist

- [ ] SD1.5 å„ªåŒ–å·²å®Œæˆï¼Œæœ€ä½³åƒæ•¸å·²æå–
- [ ] SDXL base model å·²ä¸‹è¼‰
- [ ] æ•¸æ“šé›†è·¯å¾‘æª¢æŸ¥ï¼ˆä½¿ç”¨ç›¸åŒçš„ SD1.5 æ•¸æ“šé›†ï¼‰
- [ ] VRAM å……è¶³ï¼ˆè‡³å°‘ 16GBï¼Œå»ºè­° 24GBï¼‰
- [ ] ä½¿ç”¨ `--xformers` å’Œ `--gradient_checkpointing`
- [ ] `text_encoder_lr` è¨­ç‚º `learning_rate Ã— 0.67`
- [ ] `batch_size` é™ä½åˆ° 2-4
- [ ] `max_token_length` è¨­ç‚º 225
- [ ] ä½¿ç”¨ nohup èƒŒæ™¯é‹è¡Œ
- [ ] å•Ÿå‹• TensorBoard ç›£æ§
- [ ] æ¯ 2 epochs ä¿å­˜ checkpoint
- [ ] é ç•™ 6-10 å°æ™‚è¨“ç·´æ™‚é–“

---

## ğŸ“‚ ç›®éŒ„çµæ§‹åƒè€ƒ

```
/mnt/data/ai_data/models/lora/luca/
â”œâ”€â”€ optimization_overnight/           # SD1.5 å„ªåŒ–çµæœ
â”‚   â”œâ”€â”€ trial_0025/                  # å‡è¨­æœ€ä½³ trial
â”‚   â”‚   â”œâ”€â”€ params.json              # æå–è¶…åƒæ•¸
â”‚   â”‚   â””â”€â”€ lora_trial_25.safetensors
â”‚   â””â”€â”€ CONVERGENCE_ALERT.txt
â”‚
â””â”€â”€ sdxl_v1/                         # SDXL è¨“ç·´çµæœ
    â”œâ”€â”€ luca_sdxl_v1-000002.safetensors  (epoch 2)
    â”œâ”€â”€ luca_sdxl_v1-000004.safetensors  (epoch 4)
    â”œâ”€â”€ luca_sdxl_v1-000006.safetensors  (epoch 6)
    â”œâ”€â”€ luca_sdxl_v1-000008.safetensors  (epoch 8)
    â”œâ”€â”€ luca_sdxl_v1-000010.safetensors  (epoch 10)
    â”œâ”€â”€ luca_sdxl_v1.safetensors  (final, epoch 12)
    â”œâ”€â”€ training.log
    â”œâ”€â”€ logs/  (TensorBoard)
    â””â”€â”€ eval_epoch8/  (æ¸¬è©¦åœ–ç‰‡)
```

---

## ğŸš€ å¿«é€Ÿæ±ºç­–æ¨¹

```
SD1.5 å„ªåŒ–å®Œæˆï¼Ÿ
 â”œâ”€ æ˜¯ â†’ æå–æœ€ä½³åƒæ•¸ â†’ å•Ÿå‹• SDXL è¨“ç·´
 â””â”€ å¦ â†’ ç­‰å¾…å®Œæˆï¼ˆç›£æ§æ”¶æ–‚ç‹€æ…‹ï¼‰

VRAM å……è¶³ï¼ˆâ‰¥16GBï¼‰ï¼Ÿ
 â”œâ”€ æ˜¯ â†’ batch_size=4
 â””â”€ å¦ â†’ batch_size=2 + gradient_accumulation_steps=4

éœ€è¦æ¥µè‡´è³ªé‡ï¼Ÿ
 â”œâ”€ æ˜¯ â†’ SDXLï¼ˆ1024Ã—1024ï¼‰
 â””â”€ å¦ â†’ SD1.5ï¼ˆ512Ã—512ï¼‰è¶³å¤ 

æ™‚é–“ç·Šè¿«ï¼Ÿ
 â”œâ”€ æ˜¯ â†’ ä½¿ç”¨ SD1.5ï¼ˆå¿« 2-3xï¼‰
 â””â”€ å¦ â†’ ä½¿ç”¨ SDXLï¼ˆè³ªé‡æ›´å¥½ï¼‰
```

---

## ğŸ“ éœ€è¦å¹«åŠ©ï¼Ÿ

**è©³ç´°æŒ‡å—**: æŸ¥çœ‹ `SD15_TO_SDXL_MIGRATION.md`
**ç›£æ§è…³æœ¬**: ä½¿ç”¨ `monitor_optimization_progress.sh`
**è©•ä¼°å·¥å…·**: ä½¿ç”¨ `evaluate_single_checkpoint.py`

---

**æœ€å¾Œæ›´æ–°**: 2025-11-12
**ç‰ˆæœ¬**: 1.0
