# V2.1 Optimization Script Implementation Guide

**Date:** 2025-11-13
**Version:** 2.1
**Base Script:** `/mnt/c/AI_LLM_projects/3d-animation-lora-pipeline/scripts/optimization/optuna_hyperparameter_search.py`

---

## üìã Overview

This guide outlines the specific code changes needed to implement V2.1 optimization strategy, which:
- Narrows learning rate ranges based on Trial 1-5 analysis
- Uses Alpha/Dim ratio (0.25-0.9) instead of absolute Alpha values
- Adds stratified safety constraints for high-alpha configurations
- Increases minimum training epochs

---

## üîß Code Modifications

### 1. Update `suggest_hyperparameters()` Method

**Location:** Line 67-121 in `optuna_hyperparameter_search.py`

#### 1.1 Learning Rate Ranges (NARROWED)

```python
# BEFORE (V1):
"learning_rate": trial.suggest_float("learning_rate", 5e-5, 2e-4, log=True),
"text_encoder_lr": trial.suggest_float("text_encoder_lr", 3e-5, 1e-4, log=True),

# AFTER (V2.1):
"learning_rate": trial.suggest_float("learning_rate", 6e-5, 1.2e-4, log=True),  # Narrowed from 5e-5~2e-4
"text_encoder_lr": trial.suggest_float("text_encoder_lr", 3e-5, 8e-5, log=True),  # Narrowed from 1e-4
```

#### 1.2 Network Alpha - Use Ratio Instead of Absolute Values

```python
# BEFORE (V1):
"network_dim": trial.suggest_categorical("network_dim", [64, 96, 128, 192, 256]),
"network_alpha": trial.suggest_categorical("network_alpha", [32, 48, 64, 96, 128]),

# AFTER (V2.1):
"network_dim": trial.suggest_categorical("network_dim", [64, 128, 256]),  # Simplified choices
"network_alpha_ratio": trial.suggest_float("network_alpha_ratio", 0.25, 0.9, step=0.05),

# Calculate actual alpha AFTER getting dim
network_dim = params["network_dim"]
alpha_ratio = params["network_alpha_ratio"]
network_alpha = int(network_dim * alpha_ratio)
params["network_alpha"] = network_alpha
```

#### 1.3 Training Epochs (INCREASED MINIMUM)

```python
# BEFORE (V1):
"max_train_epochs": trial.suggest_categorical("max_train_epochs", [8, 10, 12, 15]),

# AFTER (V2.1):
"max_train_epochs": trial.suggest_categorical("max_train_epochs", [12, 16, 20]),  # Removed 8, 10
```

#### 1.4 Remove Prodigy from Optimizer Choices

```python
# BEFORE (V1):
"optimizer_type": trial.suggest_categorical("optimizer_type", ["AdamW", "AdamW8bit", "Lion"]),

# AFTER (V2.1):
"optimizer_type": trial.suggest_categorical("optimizer_type", ["AdamW", "AdamW8bit"]),
# Removed Lion - unreliable in practice, focus on AdamW variants
```

---

### 2. Add Safety Constraint Checker Function

**Location:** Add new method after `suggest_hyperparameters()` (around line 122)

```python
def check_safety_constraints(self, params: Dict[str, any]) -> Tuple[bool, str]:
    """
    Check if parameter combination is safe to train

    Returns:
        (is_valid, rejection_reason)
    """
    lr = params["learning_rate"]
    text_lr = params["text_encoder_lr"]
    dim = params["network_dim"]
    alpha = params["network_alpha"]
    alpha_ratio = params["network_alpha_ratio"]
    optimizer = params["optimizer_type"]
    epochs = params["max_train_epochs"]
    grad_accum = params["gradient_accumulation_steps"]
    warmup = params["lr_warmup_steps"]

    # Constraint 1: High LR + 8bit optimizer
    if lr > 0.00012 and optimizer == "AdamW8bit":
        return False, "High LR (>0.00012) + AdamW8bit = unstable"

    # Constraint 2: Exact Alpha = Dim (avoid float comparison issues)
    if abs(alpha - dim) < 1:  # Within 1 of being equal
        return False, "Alpha ‚âà Dim causes overfitting + instability"

    # Constraint 3: High Dim + Few Epochs
    if dim >= 256 and epochs < 16:
        return False, "High dim (‚â•256) needs ‚â•16 epochs for convergence"

    # Constraint 4: Very high LR + low warmup
    if lr > 0.0001 and warmup < 100:
        return False, "High LR (>0.0001) needs ‚â•100 warmup steps"

    # Constraint 5: Gradient Accumulation 1 + High LR
    if grad_accum == 1 and lr > 0.00011:
        return False, "High LR needs gradient accumulation ‚â•2"

    # === NEW: Stratified Constraints for High Alpha Ratios ===

    # Constraint 6: High memory (ratio >= 0.75) requires strong stability
    if alpha_ratio >= 0.75:
        required_checks = [
            ("epochs >= 16", epochs >= 16),
            ("grad_accum >= 2", grad_accum >= 2),
            ("optimizer = AdamW", optimizer == "AdamW"),  # No 8bit with high alpha
            ("warmup >= 150", warmup >= 150),
        ]

        failed_checks = [name for name, passed in required_checks if not passed]
        if failed_checks:
            return False, f"High alpha (‚â•0.75) requires: {', '.join(failed_checks)}"

    # Constraint 7: Very high alpha (>= 0.85) + high LR is dangerous
    if alpha_ratio >= 0.85 and lr > 0.0001:
        return False, "Very high alpha (‚â•0.85) + LR >0.0001 = explosion risk"

    return True, ""
```

---

### 3. Modify `train_lora()` to Use Safety Constraints

**Location:** Line 123-149

```python
def train_lora(self, trial: Trial, params: Dict[str, any]) -> Path:
    """
    Train LoRA with given hyperparameters

    Returns:
        Path to trained checkpoint
    """
    # === ADD THIS SECTION AT THE START ===
    # Check safety constraints BEFORE training
    is_valid, rejection_reason = self.check_safety_constraints(params)
    if not is_valid:
        print(f"\n‚ùå TRIAL REJECTED: {rejection_reason}")
        print(f"Parameters: {json.dumps(params, indent=2)}\n")
        raise optuna.TrialPruned(rejection_reason)

    # === ORIGINAL CODE CONTINUES ===
    self.trial_counter += 1
    trial_dir = self.output_dir / f"trial_{self.trial_counter:04d}"
    # ... rest of method unchanged
```

---

### 4. Update Parameter Logging

**Location:** Around line 138-144 (inside `train_lora`)

```python
# BEFORE:
print(f"Parameters:")
for key, value in params.items():
    print(f"  {key}: {value}")

# AFTER (add alpha ratio info):
print(f"Parameters:")
for key, value in params.items():
    if key == "network_alpha":
        alpha_ratio = params.get("network_alpha_ratio", value / params["network_dim"])
        print(f"  {key}: {value} (ratio: {alpha_ratio:.3f})")
    else:
        print(f"  {key}: {value}")
```

---

### 5. Remove Old Alpha Constraint Logic

**Location:** Line 117-119

```python
# REMOVE THIS (V1):
# Ensure network_alpha <= network_dim (common constraint)
if params["network_alpha"] > params["network_dim"]:
    params["network_alpha"] = params["network_dim"]

# NOT NEEDED IN V2.1 - Alpha is calculated from ratio, guaranteed to be ‚â§ dim * 0.9
```

---

## üìä Expected Search Space Distribution

With these changes, for 20 trials:

| Parameter | V1 Distribution | V2.1 Distribution |
|-----------|----------------|-------------------|
| **LR Range** | 5e-5 ~ 2e-4 (wide) | 6e-5 ~ 1.2e-4 (focused) |
| **Alpha Ratio** | N/A (absolute) | 0.25-0.9 continuous |
| **Low Alpha (0.25-0.4)** | ~30% | ~30% |
| **Mid Alpha (0.5-0.7)** | ~30% | ~40% (core exploration) |
| **High Alpha (0.75-0.9)** | ~10% | ~30% (Trial 4 range) |
| **Rejected Trials** | ~30-40% | <10% (with constraints) |
| **Min Epochs** | 8 | 12 ‚úÖ |

---

## üöÄ Usage Example

```bash
# V2.1 Optimizati on Run (20 trials, conservative phase)
conda run -n kohya_ss python \
  /mnt/c/AI_LLM_projects/3d-animation-lora-pipeline/scripts/optimization/optuna_hyperparameter_search.py \
  --dataset-config /mnt/c/AI_LLM_projects/3d-animation-lora-pipeline/configs/training/luca_human_dataset.toml \
  --base-model /mnt/c/AI_LLM_projects/ai_warehouse/models/stable-diffusion/checkpoints/v1-5-pruned-emaonly.safetensors \
  --output-dir /mnt/data/ai_data/models/lora/luca/optimization_v2.1 \
  --study-name luca_v2.1_optimization \
  --n-trials 20 \
  --device cuda
```

---

## ‚úÖ Verification Checklist

After implementing changes:

- [ ] Script runs without syntax errors
- [ ] First trial prints alpha ratio in parameters
- [ ] Trials with alpha_ratio >= 0.75 are checked for all stability requirements
- [ ] Rejected trials show clear rejection reason
- [ ] Learning rate range is [6e-5, 1.2e-4]
- [ ] All trials use epochs ‚â• 12
- [ ] database and logs are created properly

---

## üìù Summary of Changes

**V2.1 vs V1:**

1. ‚úÖ Narrowed LR ranges (based on Trial 1-5 data)
2. ‚úÖ Alpha ratio approach (0.25-0.9) instead of categorical
3. ‚úÖ Stratified safety constraints for high alpha
4. ‚úÖ Increased minimum epochs (12 vs 8)
5. ‚úÖ Simplified dim choices (3 vs 5 options)
6. ‚úÖ Removed unreliable optimizer (Lion)
7. ‚úÖ Pre-training safety checks

**Expected Improvements:**
- Stable trials: 40-50% ‚Üí 80-90%
- Quality failures: 30-40% ‚Üí <10%
- Optimal configs found: 10-15% ‚Üí 30-40%

---

**End of Implementation Guide**
