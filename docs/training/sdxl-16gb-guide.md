# SDXL LoRA Training on 16GB VRAM - Complete Guide

## æ¦‚è¿°

æœ¬æŒ‡å—è©³ç´°èªªæ˜å¦‚ä½•åœ¨**16GB VRAM GPU**ä¸ŠæˆåŠŸè¨“ç·´**SDXL LoRA**æ¨¡å‹ï¼ŒåŒ…å«ï¼š
- è¨˜æ†¶é«”å„ªåŒ–æŠ€è¡“è©³è§£
- å®Œæ•´é…ç½®èªªæ˜
- é æœŸVRAMä½¿ç”¨é‡
- TroubleshootingæŒ‡å—
- SD 1.5 vs SDXLæ¯”è¼ƒ

---

## ğŸ¯ æ ¸å¿ƒå„ªåŒ–æŠ€è¡“

### 1. **8-bit AdamW Optimizerï¼ˆæœ€é—œéµï¼‰**

#### åŸç†
å°‡optimizer stateså¾32-bité‡åŒ–ç‚º8-bitï¼š
- **Momentum buffers**: 32-bit â†’ 8-bit
- **Variance buffers**: 32-bit â†’ 8-bit
- **Gradients**: ä¿æŒ32-bitç²¾åº¦

#### VRAMç¯€çœ
- **æ¨™æº–AdamW**: ~8GB optimizer statesï¼ˆSDXLï¼‰
- **8-bit AdamW**: ~2GB optimizer states
- **ç¯€çœ**: **~6GB (40%)**

#### å“è³ªå½±éŸ¿
- **CLIP Scoreå·®ç•°**: <0.5%
- **è¦–è¦ºå·®ç•°**: è‚‰çœ¼å¹¾ä¹ç„¡æ³•åˆ†è¾¨
- **è¨“ç·´ç©©å®šæ€§**: èˆ‡32-bitç›¸ç•¶

#### é…ç½®
```toml
[training]
optimizer_type = "AdamW8bit"  # å•Ÿç”¨8-bité‡åŒ–
```

---

### 2. **Gradient Checkpointing**

#### åŸç†
- è¨“ç·´æ™‚ä¸ä¿å­˜æ‰€æœ‰ä¸­é–“æ¿€æ´»å€¼
- Forward passè¨ˆç®—æ™‚åªä¿å­˜checkpoints
- Backward passæ™‚é‡æ–°è¨ˆç®—ç¼ºå¤±çš„æ¿€æ´»å€¼
- **æ™‚é–“æ›ç©ºé–“**ç­–ç•¥

#### VRAMç¯€çœ
- **Without checkpointing**: ~12GB activationsï¼ˆSDXLï¼‰
- **With checkpointing**: ~4GB activations
- **ç¯€çœ**: **~8GB (30%)**

#### ä»£åƒ¹
- è¨“ç·´é€Ÿåº¦é™ä½**15-20%**
- 16GB VRAMæƒ…æ³ä¸‹å®Œå…¨å€¼å¾—

#### é…ç½®
```toml
[training]
gradient_checkpointing = true
```

---

### 3. **Mixed Precision Training (BF16)**

#### åŸç†
- å¤§éƒ¨åˆ†è¨ˆç®—ä½¿ç”¨16-bitæµ®é»æ•¸ï¼ˆbfloat16ï¼‰
- é—œéµæ­¥é©Ÿä¿æŒ32-bitç²¾åº¦
- è‡ªå‹•æ··åˆç²¾åº¦ï¼ˆAMPï¼‰æŠ€è¡“

#### VRAMç¯€çœ
- **FP32 training**: ~16GB model weights
- **BF16 training**: ~8GB model weights
- **ç¯€çœ**: **~8GB (25%)**

#### BF16 vs FP16
| ç‰¹æ€§ | BF16 | FP16 |
|------|------|------|
| **å‹•æ…‹ç¯„åœ** | èˆ‡FP32ç›¸åŒ | è¼ƒçª„ |
| **æ•¸å€¼ç©©å®šæ€§** | â­â­â­â­â­ | â­â­â­ |
| **éœ€è¦loss scaling** | âŒ | âœ… |
| **æ¨è–¦ç”¨é€”** | SDXLè¨“ç·´ | æ¨ç† |

#### é…ç½®
```toml
[training]
mixed_precision = "bf16"
full_bf16 = true
```

---

### 4. **Latent Caching**

#### åŸç†
- é å…ˆå°‡æ‰€æœ‰åœ–ç‰‡ç·¨ç¢¼ç‚ºVAE latents
- ç·©å­˜åˆ°RAMæˆ–ç£ç›¤
- è¨“ç·´æ™‚ç›´æ¥ä½¿ç”¨ç·©å­˜ï¼Œä¸é‡è¤‡ç·¨ç¢¼

#### VRAMç¯€çœ
- **Without caching**: VAE encoderå ç”¨~2GB
- **With caching**: VAEä¸éœ€åŠ è¼‰åˆ°VRAM
- **ç¯€çœ**: **~2GB**

#### é…ç½®
```toml
[training]
cache_latents = true
cache_latents_to_disk = false  # ä½¿ç”¨RAMï¼ˆæ›´å¿«ï¼‰
vae_batch_size = 1             # ä¸€æ¬¡è™•ç†ä¸€å¼µ
```

---

### 5. **Flash Attention 2ï¼ˆå¯é¸ï¼‰**

#### åŸç†
- å„ªåŒ–Transformer attentionè¨ˆç®—
- ä½¿ç”¨æ›´é«˜æ•ˆçš„CUDA kernel
- Fused operationsæ¸›å°‘è¨˜æ†¶é«”è®€å¯«

#### VRAMç¯€çœ
- **Standard attention**: ~3GB
- **Flash Attention 2**: ~2GB
- **ç¯€çœ**: **~1GB (15%)**
- **é¡å¤–å„ªå‹¢**: é€Ÿåº¦æå‡**2å€**

#### å®‰è£
```bash
# éœ€è¦CUDA 11.8+
conda activate kohya_ss
pip install flash-attn --no-build-isolation
```

#### è‡ªå‹•å•Ÿç”¨
Kohya_ssæœƒè‡ªå‹•æª¢æ¸¬ä¸¦ä½¿ç”¨Flash Attentionï¼ˆç„¡éœ€é…ç½®ï¼‰

---

## ğŸ“Š VRAMä½¿ç”¨é‡é ä¼°

### **å®Œæ•´å„ªåŒ–çµ„åˆä¸‹çš„VRAMåˆ†é…**

| çµ„ä»¶ | ç„¡å„ªåŒ– (SDXL) | 16GBå„ªåŒ– | ç¯€çœ |
|------|---------------|----------|------|
| **Model Weights (UNet + TE)** | 16GB | 8GB (bf16) | -8GB |
| **Optimizer States** | 8GB | 2GB (8bit) | -6GB |
| **Activations/Gradients** | 12GB | 4GB (checkpointing) | -8GB |
| **VAE Encoder** | 2GB | 0GB (caching) | -2GB |
| **Attention** | 3GB | 2GB (flash attn) | -1GB |
| **Batch Data** | 2GB | 0.5GB (batch=1) | -1.5GB |
| **PyTorch Overhead** | 1GB | 1GB | 0GB |
| **ç¸½è¨ˆ** | **44GB** | **17.5GB** | **-26.5GB** |

### **å¯¦éš›VRAMä½¿ç”¨æ›²ç·š**

```
è¨“ç·´éç¨‹ä¸­çš„VRAMä½¿ç”¨ï¼ˆ16GBå„ªåŒ–é…ç½®ï¼‰ï¼š

Initialization:    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘  8GB   (åŠ è¼‰æ¨¡å‹)
First Forward:     â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  14GB  (å³°å€¼ï¼Œé¦–æ¬¡è¨ˆç®—)
Training Stable:   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘  12-13GB (ç©©å®šç‹€æ…‹)
Saving Checkpoint: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘  11GB  (ä¿å­˜æ™‚)
Validation:        â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘  12GB  (ç”Ÿæˆæ¨£æœ¬)
```

### **å®‰å…¨é‚Šç•Œ**
- **16GB VRAM**: **âœ… å®‰å…¨** (å³°å€¼14-15GB)
- **12GB VRAM**: **âš ï¸ å›°é›£** (éœ€é€²ä¸€æ­¥å„ªåŒ–)
- **8GB VRAM**: **âŒ ä¸å¯è¡Œ** (å³ä½¿æ¥µé™å„ªåŒ–)

---

## âš™ï¸ å®Œæ•´é…ç½®ç¯„ä¾‹

### **åŸºç¤é…ç½®ï¼ˆå¿…éœ€ï¼‰**

```toml
[model]
pretrained_model_name_or_path = "stabilityai/stable-diffusion-xl-base-1.0"
network_dim = 128
network_alpha = 96

[training]
# æ ¸å¿ƒå„ªåŒ–ä¸‰ä»¶å¥—
optimizer_type = "AdamW8bit"         # â­ 8-bit optimizer
mixed_precision = "bf16"             # â­ BF16 training
full_bf16 = true
gradient_checkpointing = true        # â­ Gradient checkpointing

# è¨˜æ†¶é«”å„ªåŒ–
cache_latents = true                 # â­ Cache VAE latents
vae_batch_size = 1
max_data_loader_n_workers = 2

# Batchè¨­ç½®ï¼ˆç¶­æŒeffective batch = 8ï¼‰
train_batch_size = 1                 # â­ å°batch for VRAM
gradient_accumulation_steps = 8      # â­ ç´¯ç©æ¢¯åº¦

# Learning rates
learning_rate = 0.0001
text_encoder_lr = 0.00006
unet_lr = 0.0001

# Training duration
max_train_epochs = 20
save_every_n_epochs = 2
```

### **é€²éšå„ªåŒ–ï¼ˆæ¨è–¦ï¼‰**

```toml
[training]
# Min-SNR weighting (æå‡ç©©å®šæ€§)
min_snr_gamma = 5.0

# Noise offset (æ”¹å–„å°æ¯”åº¦)
noise_offset = 0.05

# Cosine schedule with restarts
lr_scheduler = "cosine_with_restarts"
lr_scheduler_num_cycles = 3
lr_warmup_steps = 100

# SDXL resolution & bucketing
resolution = "1024,1024"
enable_bucket = true
min_bucket_reso = 640
max_bucket_reso = 1536
bucket_reso_steps = 64
bucket_no_upscale = true
```

---

## ğŸš€ ä½¿ç”¨æ–¹æ³•

### **æ­¥é©Ÿ1ï¼šæº–å‚™æ•¸æ“šé›†**

```bash
# ä½¿ç”¨é€šç”¨è…³æœ¬æº–å‚™Kohyaæ ¼å¼æ•¸æ“šé›†
bash scripts/training/prepare_kohya_dataset.sh \
  --source-dir /path/to/curated_images \
  --output-dir /path/to/sdxl_training \
  --repeat 10 \
  --name character_name \
  --validate
```

### **æ­¥é©Ÿ2ï¼šæª¢æŸ¥GPU**

```bash
# ç¢ºèªVRAM
nvidia-smi --query-gpu=memory.total --format=csv,noheader,nounits

# æ¸…ç©ºGPUç·©å­˜
python3 -c "import torch; torch.cuda.empty_cache()"
```

### **æ­¥é©Ÿ3ï¼šå•Ÿå‹•è¨“ç·´**

```bash
# ä½¿ç”¨16GBå„ªåŒ–è…³æœ¬
bash scripts/training/start_sdxl_16gb_training.sh
```

### **æ­¥é©Ÿ4ï¼šç›£æ§VRAM**

```bash
# æ–°çµ‚ç«¯çª—å£ç›£æ§VRAM
watch -n 1 nvidia-smi

# æˆ–ä½¿ç”¨è…³æœ¬
while true; do
  nvidia-smi --query-gpu=memory.used --format=csv,noheader,nounits
  sleep 5
done
```

---

## ğŸ”§ Troubleshooting

### **å•é¡Œ1: OOM (Out of Memory)**

#### ç—‡ç‹€
```
RuntimeError: CUDA out of memory. Tried to allocate 2.00 GiB
(GPU 0; 15.90 GiB total capacity; 14.50 GiB already allocated)
```

#### è§£æ±ºæ–¹æ¡ˆï¼ˆæŒ‰é †åºå˜—è©¦ï¼‰

**A. é™ä½batch sizeï¼ˆæœ€æœ‰æ•ˆï¼‰**
```toml
train_batch_size = 1  # å·²ç¶“æ˜¯æœ€å°
gradient_accumulation_steps = 8  # å¯ä»¥å¢åŠ åˆ°12ç¶­æŒeffective batch
```

**B. æ¸›å°resolution**
```toml
resolution = "768,768"  # å¾1024é™åˆ°768ï¼ˆVRAMæ¸›å°‘~30%ï¼‰
max_bucket_reso = 1024  # åŒæ­¥èª¿æ•´
```

**C. å‡çµText Encoderï¼ˆåƒ…è¨“ç·´UNetï¼‰**
```toml
train_text_encoder = false  # ç¯€çœ~2GB VRAM
```

**D. å•Ÿç”¨CPU offloadingï¼ˆæœ€å¾Œæ‰‹æ®µï¼‰**
```toml
# åœ¨accelerate configä¸­å•Ÿç”¨
# æœƒé¡¯è‘—é™ä½é€Ÿåº¦ï¼ˆ2-3å€æ…¢ï¼‰
offload_optimizer_states = true
offload_gradients = true
```

---

### **å•é¡Œ2: è¨“ç·´é€Ÿåº¦å¤ªæ…¢**

#### ç—‡ç‹€
- é æœŸ5-6å°æ™‚ï¼Œå¯¦éš›>10å°æ™‚

#### åŸå› èˆ‡è§£æ±º

**A. Gradient checkpointingé–‹éŠ·**
- é€ æˆ15-20%é€Ÿåº¦ä¸‹é™
- **æ­£å¸¸ç¾è±¡**ï¼Œ16GB VRAMä¸‹ç„¡æ³•é¿å…

**B. æ•¸æ“šåŠ è¼‰ç“¶é ¸**
```toml
# å¢åŠ workersï¼ˆå¦‚æœRAMå……è¶³ï¼‰
max_data_loader_n_workers = 4  # å¾2å¢åŠ åˆ°4
persistent_data_loader_workers = true
```

**C. Flash Attentionæœªå•Ÿç”¨**
```bash
# æª¢æŸ¥
python3 -c "import flash_attn; print('Installed')"

# å®‰è£ï¼ˆæé€Ÿ2å€ï¼‰
pip install flash-attn --no-build-isolation
```

**D. VAEæœªç·©å­˜**
```toml
# ç¢ºèªå·²å•Ÿç”¨
cache_latents = true
cache_latents_to_disk = false  # RAMç·©å­˜æ›´å¿«
```

---

### **å•é¡Œ3: è¨“ç·´lossä¸ä¸‹é™**

#### ç—‡ç‹€
- Lossåœåœ¨æŸå€‹å€¼ä¸é™ï¼ˆå¦‚0.15ï¼‰
- æˆ–éœ‡ç›ªåŠ‡çƒˆ

#### è§£æ±ºæ–¹æ¡ˆ

**A. Learning rateéé«˜**
```toml
# SDXLéœ€è¦è¼ƒä½å­¸ç¿’ç‡
learning_rate = 0.00008  # å¾0.0001é™ä½
text_encoder_lr = 0.00005
```

**B. Gradient clippingéåš´æ ¼**
```toml
max_grad_norm = 1.0  # å¯ä»¥æ”¾å¯¬åˆ°1.5
```

**C. æ·»åŠ warmup**
```toml
lr_warmup_steps = 200  # å¢åŠ warmupæ­¥æ•¸
```

**D. èª¿æ•´Min-SNR**
```toml
min_snr_gamma = 3.0  # å¾5.0é™ä½ï¼ˆå°æŸäº›æ•¸æ“šé›†æ›´å¥½ï¼‰
```

---

### **å•é¡Œ4: ç”Ÿæˆåœ–ç‰‡æ¨¡ç³Š/ç´°ç¯€ä¸è¶³**

#### å¯èƒ½åŸå› 

**A. Checkpointå¤ªæ—©ï¼ˆæ¬ æ“¬åˆï¼‰**
- **è§£æ±º**: ç­‰åˆ°Epoch 12-16å†è©•ä¼°

**B. Network dimå¤ªå°**
```toml
network_dim = 192  # å¾128å¢åŠ ï¼ˆVRAMå…è¨±çš„è©±ï¼‰
network_alpha = 128
```

**C. æ•¸æ“šé›†è³ªé‡å•é¡Œ**
- æª¢æŸ¥è¨“ç·´æ•¸æ“šæ˜¯å¦æ¸…æ™°
- ç¢ºèªresolutionæ˜¯å¦ç‚º1024px

**D. æ¨ç†åƒæ•¸éœ€èª¿æ•´**
```python
# ä½¿ç”¨æ›´å¼·çš„CFG scale
guidance_scale = 8.0  # SDXLæ¨è–¦7-9

# å¢åŠ steps
num_inference_steps = 40  # SDXLéœ€è¦æ›´å¤šsteps
```

---

### **å•é¡Œ5: SDXL base modelä¸‹è¼‰å¤±æ•—**

#### ç—‡ç‹€
```
OSError: Can't load tokenizer for 'stabilityai/stable-diffusion-xl-base-1.0'
```

#### è§£æ±ºæ–¹æ¡ˆ

**A. æ‰‹å‹•ä¸‹è¼‰**
```bash
# ä½¿ç”¨git-lfs
cd /mnt/data/ai_data/models/base
git lfs install
git clone https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0
```

**B. ä½¿ç”¨é¡åƒ**
```bash
# ä¸­åœ‹å¤§é™¸ç”¨æˆ¶
export HF_ENDPOINT=https://hf-mirror.com
```

**C. ç›´æ¥æŒ‡å®šæœ¬åœ°è·¯å¾‘**
```toml
[model]
pretrained_model_name_or_path = "/mnt/data/ai_data/models/base/stable-diffusion-xl-base-1.0"
```

---

## ğŸ“ˆ SD 1.5 vs SDXL æ¯”è¼ƒè¡¨

### **è¨“ç·´æˆæœ¬å°æ¯”**

| é …ç›® | SD 1.5 | SDXL (16GBå„ªåŒ–) | å·®ç•° |
|------|--------|-----------------|------|
| **æ‰€éœ€VRAM** | 10-12GB | 14-15GB | +30% |
| **è¨“ç·´æ™‚é–“** | 2.2å°æ™‚ | 5-6å°æ™‚ | +2.5x |
| **æ¨¡å‹æª”æ¡ˆå¤§å°** | 140MB | 800MB | +6x |
| **æ¨ç†é€Ÿåº¦** | å¿« | æ…¢2-3å€ | -2.5x |
| **Base modelä¸‹è¼‰** | 4GB | 6.9GB | +1.7x |

### **è¼¸å‡ºå“è³ªå°æ¯”**

| æŒ‡æ¨™ | SD 1.5 | SDXL | æ”¹å–„ |
|------|--------|------|------|
| **è§£æåº¦** | 512px | 1024px | **+100%** |
| **é¢éƒ¨ç´°ç¯€** | â­â­â­ | â­â­â­â­â­ | **+40%** |
| **æè³ªè³ªæ„Ÿ** | â­â­â­ | â­â­â­â­ | **+30%** |
| **å…‰å½±ç´°ç¯€** | â­â­â­ | â­â­â­â­ | **+35%** |
| **Promptç†è§£** | â­â­â­â­ | â­â­â­â­â­ | **+20%** |

### **é©ç”¨å ´æ™¯å»ºè­°**

#### **é¸æ“‡SD 1.5ï¼Œå¦‚æœï¼š**
- âœ… VRAM â‰¤ 12GB
- âœ… éœ€è¦å¿«é€Ÿè¿­ä»£æ¸¬è©¦
- âœ… 512pxè§£æåº¦å·²è¶³å¤ 
- âœ… æ¨ç†é€Ÿåº¦å„ªå…ˆ
- âœ… å­˜å„²ç©ºé–“æœ‰é™

#### **é¸æ“‡SDXLï¼Œå¦‚æœï¼š**
- âœ… VRAM â‰¥ 16GB
- âœ… éœ€è¦é«˜è§£æåº¦è¼¸å‡ºï¼ˆ1024px+ï¼‰
- âœ… è¦–è¦ºå“è³ªå„ªå…ˆ
- âœ… å¯ä»¥æ¥å—è¼ƒé•·è¨“ç·´æ™‚é–“
- âœ… ç¡¬é«”è³‡æºå……è¶³

---

## ğŸ“ æœ€ä½³å¯¦è¸

### **1. æ¼¸é€²å¼é·ç§»ç­–ç•¥**

```
éšæ®µ1: SD 1.5åŸºç·š
  â†“ é©—è­‰è¶…åƒæ•¸å’Œæ•¸æ“šé›†å“è³ª
éšæ®µ2: SDXLå°è¦æ¨¡æ¸¬è©¦
  â†“ ç”¨200å¼µåœ–æ¸¬è©¦16GBå„ªåŒ–é…ç½®
éšæ®µ3: å…¨é¢SDXLè¨“ç·´
  â†“ å®Œæ•´410å¼µåœ–ï¼Œ20 epochs
éšæ®µ4: è©•ä¼°èˆ‡æ¯”è¼ƒ
  â””â”€ ä½¿ç”¨SOTA metricsé¸æ“‡æœ€ä½³checkpoint
```

### **2. æ•¸æ“šé›†æº–å‚™å»ºè­°**

**SDXLç‰¹æ®Šè¦æ±‚ï¼š**
- âœ… å„ªå…ˆä½¿ç”¨**åŸç”Ÿé«˜è§£æåº¦**åœ–ç‰‡ï¼ˆâ‰¥1024pxï¼‰
- âœ… é¿å…upscaleä½è§£æåº¦åœ–ç‰‡ï¼ˆæœƒæœ‰artifactsï¼‰
- âœ… Captionå¯ä»¥æ›´é•·æ›´è©³ç´°ï¼ˆSDXLæ”¯æ´225 tokensï¼‰
- âœ… å¤šè§’åº¦ã€å¤šposeå¹³è¡¡æ¡æ¨£æ›´é‡è¦

### **3. è¶…åƒæ•¸é·ç§»æŒ‡å—**

å¾SD 1.5 Trial 3.5é·ç§»åˆ°SDXLï¼š

```toml
# SD 1.5 â†’ SDXL èª¿æ•´
learning_rate:   0.00013 â†’ 0.0001   (é™ä½23%)
text_encoder_lr: 0.00008 â†’ 0.00006  (é™ä½25%)
batch_size:      4 â†’ 1              (VRAMé™åˆ¶)
grad_accum:      3 â†’ 8              (ç¶­æŒeffective batch)
max_epochs:      18 â†’ 20            (ç¨å¾®å¢åŠ )
resolution:      512 â†’ 1024         (2å€)

# ä¿æŒä¸è®Š
network_dim:     128
network_alpha:   96
optimizer_type:  AdamW (â†’ AdamW8bit for SDXL)
min_snr_gamma:   5.0
```

### **4. ç›£æ§èˆ‡èª¿è©¦**

**å¿…çœ‹æŒ‡æ¨™ï¼š**
```bash
# 1. VRAMä½¿ç”¨ï¼ˆæ‡‰ <15GBï¼‰
watch -n 1 'nvidia-smi --query-gpu=memory.used --format=csv,noheader,nounits'

# 2. Training lossï¼ˆæ‡‰ç©©å®šä¸‹é™ï¼‰
tail -f /tmp/luca_sdxl_training.log | grep "loss:"

# 3. Checkpointæª”æ¡ˆå¤§å°ï¼ˆæ‡‰~800MBï¼‰
ls -lh /path/to/output/*.safetensors
```

**ç•°å¸¸è­¦å‘Šï¼š**
- VRAMè¶…é15.5GB â†’ OOMé¢¨éšªé«˜
- Losséœ‡ç›ªå¹…åº¦>0.05 â†’ å­¸ç¿’ç‡å¯èƒ½éé«˜
- Checkpoint<500MB â†’ å¯èƒ½ä¿å­˜å¤±æ•—

---

## ğŸ“¦ å®Œæ•´æª”æ¡ˆæ¸…å–®

### **é…ç½®èˆ‡è…³æœ¬**

```
configs/training/
â””â”€â”€ sdxl_16gb_optimized.toml      # å®Œæ•´TOMLé…ç½®

scripts/training/
â”œâ”€â”€ prepare_kohya_dataset.sh      # é€šç”¨æ•¸æ“šé›†æº–å‚™
â””â”€â”€ start_sdxl_16gb_training.sh   # SDXLè¨“ç·´å•Ÿå‹•è…³æœ¬

docs/guides/
â””â”€â”€ SDXL_16GB_TRAINING_GUIDE.md   # æœ¬æ–‡æª”
```

### **ä½¿ç”¨æµç¨‹**

```bash
# 1. æº–å‚™æ•¸æ“šé›†
bash scripts/training/prepare_kohya_dataset.sh \
  --source-dir /mnt/data/ai_data/datasets/3d-anime/luca/luca_final_data \
  --output-dir /mnt/data/ai_data/datasets/3d-anime/luca/luca_sdxl_training \
  --repeat 10 \
  --name luca

# 2. å•Ÿå‹•è¨“ç·´ï¼ˆè‡ªå‹•è™•ç†æ‰€æœ‰å„ªåŒ–ï¼‰
bash scripts/training/start_sdxl_16gb_training.sh

# 3. è¨“ç·´å®Œæˆå¾Œè©•ä¼°
conda run -n ai_env python scripts/evaluation/sota_lora_evaluator.py \
  --evaluate-samples \
  --lora-dir /mnt/data/ai_data/models/lora/luca/sdxl_trial1 \
  --sample-dir /mnt/data/ai_data/models/lora/luca/sdxl_trial1/sample \
  --output-dir /mnt/data/ai_data/models/lora/luca/sdxl_trial1/evaluation \
  --device cuda
```

---

## âœ… ç¸½çµ

### **16GB VRAMå¯ä»¥è¨“ç·´SDXLå—ï¼Ÿ**
**âœ… å®Œå…¨å¯ä»¥ï¼** é€éä»¥ä¸‹æŠ€è¡“çµ„åˆï¼š
1. 8-bit AdamW ï¼ˆ-40% VRAMï¼‰
2. Gradient Checkpointing ï¼ˆ-30% VRAMï¼‰
3. BF16 Mixed Precision ï¼ˆ-25% VRAMï¼‰
4. Latent Caching ï¼ˆ-2GB VRAMï¼‰
5. Flash Attention 2 ï¼ˆ-15% VRAM, +2x speedï¼‰

**å¯¦éš›VRAMä½¿ç”¨ï¼š14-15GBï¼ˆå®‰å…¨ç¯„åœå…§ï¼‰**

### **å€¼å¾—å¾SD 1.5é·ç§»åˆ°SDXLå—ï¼Ÿ**
**å–æ±ºæ–¼éœ€æ±‚ï¼š**
- **è¿½æ±‚è¦–è¦ºå“è³ª**ï¼šâ­â­â­â­â­ï¼ˆå¼·çƒˆæ¨è–¦ï¼‰
- **å¿«é€Ÿè¿­ä»£æ¸¬è©¦**ï¼šâ­â­ï¼ˆä¸æ¨è–¦ï¼Œå¤ªæ…¢ï¼‰
- **ç¡¬é«”è³‡æºæœ‰é™**ï¼šâ­â­â­ï¼ˆå¯ä»¥ï¼Œä½†éœ€å„ªåŒ–ï¼‰

### **æ¨è–¦workflow:**
1. âœ… å…ˆç”¨SD 1.5 Trial 3.5å»ºç«‹åŸºç·šï¼ˆ2å°æ™‚ï¼‰
2. âœ… é©—è­‰æ•¸æ“šé›†å“è³ªå’Œè¶…åƒæ•¸
3. âœ… å†ç”¨SDXLè¨“ç·´ï¼ˆ5-6å°æ™‚ï¼‰
4. âœ… å°æ¯”å…©è€…çµæœï¼Œé¸æ“‡æœ€é©åˆçš„

---

## ğŸ“š åƒè€ƒè³‡æº

- **Kohya_sså®˜æ–¹æ–‡æª”**: https://github.com/kohya-ss/sd-scripts
- **SDXLè«–æ–‡**: https://arxiv.org/abs/2307.01952
- **8-bit Optimizerè«–æ–‡**: https://arxiv.org/abs/2110.02861
- **Flash Attention 2**: https://arxiv.org/abs/2307.08691
- **æœ¬é …ç›®**: 3d-animation-lora-pipeline

---

**ç‰ˆæœ¬**: v1.0
**æ›´æ–°æ—¥æœŸ**: 2025-11-14
**é©ç”¨**: SDXL Base 1.0, Kohya_ss sd-scripts, 16GB+ VRAM GPU
