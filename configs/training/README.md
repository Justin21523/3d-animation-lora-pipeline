# é…ç½®ç¯„æœ¬èªªæ˜Ž

## ðŸ“ ç¯„æœ¬æ–‡ä»¶èˆ‡ç”¨é€”

### 1. `dataset_config_template.toml` âœ… **å¯ç›´æŽ¥ä½¿ç”¨**

**ç”¨é€”ï¼š** é…åˆ Kohya çš„ `--dataset_config` åƒæ•¸ä½¿ç”¨

**æ ¼å¼ï¼š**
```toml
[general]
shuffle_caption = true
keep_tokens = 3

[[datasets]]
resolution = 512
batch_size = 10

  [[datasets.subsets]]
  image_dir = "/path/to/images"
  class_tokens = "character boy"
```

**ä½¿ç”¨æ–¹å¼ï¼š**
```bash
# 1. è¤‡è£½ç¯„æœ¬
cp configs/templates/dataset_config_template.toml configs/my_char/dataset.toml

# 2. ç·¨è¼¯é…ç½®ï¼ˆä¿®æ”¹è·¯å¾‘ã€åƒæ•¸ï¼‰
nano configs/my_char/dataset.toml

# 3. é…åˆ CLI åƒæ•¸ä½¿ç”¨
python train_network.py \
    --dataset_config configs/my_char/dataset.toml \
    --pretrained_model_name_or_path /path/to/model \
    --learning_rate 0.0001 \
    --optimizer_type AdamW8bit \
    # ... å…¶ä»–è¨“ç·´åƒæ•¸
```

**é…ç½®å…§å®¹ï¼š**
- âœ… æ•¸æ“šé›†è·¯å¾‘ (`image_dir`)
- âœ… Batch size, resolution
- âœ… Bucketing è¨­ç½®
- âœ… Caption è™•ç†é¸é …
- âœ… æ•¸æ“šå¢žå¼·é¸é …

**ä¸èƒ½é…ç½®ï¼š**
- âŒ è¨“ç·´åƒæ•¸ï¼ˆlearning rate, optimizer, epochsï¼‰
- âŒ æ¨¡åž‹è·¯å¾‘
- âŒ è¼¸å‡ºç›®éŒ„
- âŒ ç¶²çµ¡çµæ§‹ï¼ˆnetwork_dim, alphaï¼‰

---

### 2. `lora_training_template.toml` âš ï¸ **åƒ…ä½œåƒè€ƒæ–‡æª”**

**ç”¨é€”ï¼š** ä½œç‚º**å®Œæ•´è¨“ç·´é…ç½®çš„åƒè€ƒæ–‡æª”**ï¼Œè¨˜éŒ„æ‰€æœ‰å¯ç”¨åƒæ•¸

**æ ¼å¼ï¼š**
```toml
[model_arguments]
pretrained_model_name_or_path = "/path/to/model"
output_dir = "/path/to/output"

[training_arguments]
learning_rate = 0.0001
max_train_epochs = 15

[network_arguments]
network_dim = 64
network_alpha = 32

[dataset_arguments]
dataset_config = "/path/to/dataset.toml"
```

**âš ï¸ é‡è¦ï¼š** Kohya çš„ `train_network.py` **ä¸æ”¯æ´** `--config_file` åƒæ•¸ï¼Œæ‰€ä»¥é€™å€‹ç¯„æœ¬**ä¸èƒ½ç›´æŽ¥ä½¿ç”¨**ï¼

**å¯¦éš›ç”¨é€”ï¼š**
1. **åƒè€ƒæ–‡æª”** - æŸ¥é–±æ‰€æœ‰å¯ç”¨çš„è¨“ç·´åƒæ•¸
2. **é…ç½®è¦åŠƒ** - è¨ˆåŠƒè¨“ç·´é…ç½®æ™‚çš„æª¢æŸ¥æ¸…å–®
3. **æœªä¾†æ“´å±•** - å¦‚æžœå°‡ä¾†å‰µå»ºåŒ…è£å™¨è…³æœ¬æ™‚ä½¿ç”¨

**å¦‚ä½•å¯¦éš›ä½¿ç”¨é€™äº›åƒæ•¸ï¼Ÿ**

æœ‰å…©ç¨®æ–¹å¼ï¼š

#### æ–¹å¼ Aï¼šå‰µå»ºè¨“ç·´è…³æœ¬ï¼ˆæŽ¨è–¦ï¼‰

```bash
# å‰µå»ºè…³æœ¬æ–‡ä»¶
cat > configs/my_char/train.sh << 'EOF'
#!/bin/bash

# å¾ž lora_training_template.toml åƒè€ƒçš„åƒæ•¸
MODEL_PATH="/path/to/model"
OUTPUT_DIR="/path/to/output"
LEARNING_RATE="0.0001"
NETWORK_DIM="64"
EPOCHS="15"

# è¨“ç·´å‘½ä»¤
conda run -n kohya_ss python /path/to/train_network.py \
    --dataset_config configs/my_char/dataset.toml \
    --pretrained_model_name_or_path "$MODEL_PATH" \
    --output_dir "$OUTPUT_DIR" \
    --learning_rate "$LEARNING_RATE" \
    --network_dim "$NETWORK_DIM" \
    --max_train_epochs "$EPOCHS" \
    --optimizer_type "AdamW8bit" \
    --mixed_precision "fp16" \
    --gradient_checkpointing \
    --cache_latents_to_disk
EOF

chmod +x configs/my_char/train.sh
bash configs/my_char/train.sh
```

#### æ–¹å¼ Bï¼šä½¿ç”¨ Python é…ç½®å­—å…¸

```python
# train_my_char.py
import subprocess

# å¾ž lora_training_template.toml è½‰æ›ç‚º Python å­—å…¸
config = {
    # [model_arguments]
    'pretrained_model_name_or_path': '/path/to/model',
    'output_dir': '/path/to/output',
    'output_name': 'my_char_lora',
    'save_model_as': 'safetensors',

    # [training_arguments]
    'learning_rate': '0.0001',
    'max_train_epochs': '15',
    'optimizer_type': 'AdamW8bit',
    'mixed_precision': 'fp16',

    # [network_arguments]
    'network_dim': '64',
    'network_alpha': '32',
}

# æ§‹å»ºå‘½ä»¤
cmd = [
    'conda', 'run', '-n', 'kohya_ss',
    'python', '/path/to/train_network.py',
    '--dataset_config', 'configs/my_char/dataset.toml',
]

for key, value in config.items():
    cmd.extend([f'--{key}', str(value)])

subprocess.run(cmd)
```

---

## ðŸ“Š ç¯„æœ¬å°æ¯”è¡¨

| ç¯„æœ¬ | æ ¼å¼ | Kohya æ”¯æ´ | ç›´æŽ¥ä½¿ç”¨ | ç”¨é€” |
|------|------|-----------|---------|------|
| `dataset_config_template.toml` | `[general]`, `[[datasets]]` | âœ… `--dataset_config` | âœ… æ˜¯ | æ•¸æ“šé›†é…ç½® |
| `lora_training_template.toml` | `[model_arguments]`, `[training_arguments]` | âŒ ä¸æ”¯æ´ | âŒ å¦ | åƒè€ƒæ–‡æª” |

---

## ðŸŽ¯ æŽ¨è–¦å·¥ä½œæµç¨‹

### æ­¥é©Ÿ 1ï¼šè¨­ç½®æ•¸æ“šé›†é…ç½®

```bash
# è¤‡è£½ä¸¦ç·¨è¼¯æ•¸æ“šé›†é…ç½®
cp configs/templates/dataset_config_template.toml configs/my_char/dataset.toml
nano configs/my_char/dataset.toml
```

### æ­¥é©Ÿ 2ï¼šæŸ¥é–±å®Œæ•´é…ç½®ç¯„æœ¬

```bash
# æ‰“é–‹å®Œæ•´é…ç½®ç¯„æœ¬ä½œç‚ºåƒè€ƒ
cat configs/templates/lora_training_template.toml

# è¨˜ä¸‹ä½ éœ€è¦çš„åƒæ•¸åŠå…¶å€¼
```

### æ­¥é©Ÿ 3ï¼šå‰µå»ºè¨“ç·´è…³æœ¬

```bash
# æ–¹å¼ Aï¼šå‰µå»º Shell è…³æœ¬
nano configs/my_char/train.sh

# æ–¹å¼ Bï¼šå‰µå»º Python è…³æœ¬
nano configs/my_char/train.py
```

### æ­¥é©Ÿ 4ï¼šåŸ·è¡Œè¨“ç·´

```bash
# Shell è…³æœ¬
bash configs/my_char/train.sh

# Python è…³æœ¬
python configs/my_char/train.py
```

---

## ðŸ’¡ ç‚ºä»€éº¼æœ‰å…©å€‹ç¯„æœ¬ï¼Ÿ

1. **`dataset_config_template.toml`** - é€™æ˜¯ Kohya **å®˜æ–¹æ”¯æ´**çš„é…ç½®æ ¼å¼ï¼Œå¯ä»¥ç›´æŽ¥ä½¿ç”¨

2. **`lora_training_template.toml`** - é€™æ˜¯æˆ‘å€‘å‰µå»ºçš„**å®Œæ•´é…ç½®æ–‡æª”**ï¼Œé›–ç„¶ä¸èƒ½ç›´æŽ¥ç”¨æ–¼ Kohyaï¼Œä½†å®ƒï¼š
   - è¨˜éŒ„äº†æ‰€æœ‰è¨“ç·´åƒæ•¸
   - æä¾›äº†å®Œæ•´çš„é…ç½®çµæ§‹
   - æ–¹ä¾¿æŸ¥é–±å’Œè¦åŠƒ
   - å¯ç”¨æ–¼æœªä¾†çš„è‡ªå®šç¾©åŒ…è£å™¨

---

## ðŸ“š ç›¸é—œæ–‡æª”

- **è©³ç´°è§£é‡‹ï¼š** `/docs/TOML_CONFIG_EXPLAINED.md` - ç‚ºä»€éº¼æœƒæœ‰é€™å…©ç¨®æ ¼å¼
- **è¨“ç·´æŒ‡å—ï¼š** `/docs/KOHYA_TRAINING_GUIDE.md` - å®Œæ•´çš„è¨“ç·´æµç¨‹
- **å¿«é€Ÿåƒè€ƒï¼š** `/QUICK_REFERENCE_LORA.md` - å¿«é€Ÿé–‹å§‹è¨“ç·´

---

## â“ å¸¸è¦‹å•é¡Œ

### Q: ç‚ºä»€éº¼ `lora_training_template.toml` ä¸èƒ½ç›´æŽ¥ä½¿ç”¨ï¼Ÿ

**A:** Kohya çš„ `train_network.py` åªæ”¯æ´ `--dataset_config` åƒæ•¸ï¼ˆç”¨æ–¼æ•¸æ“šé›†é…ç½®ï¼‰ï¼Œä¸æ”¯æ´ `--config_file` åƒæ•¸ï¼ˆç”¨æ–¼å®Œæ•´é…ç½®ï¼‰ã€‚

### Q: é‚£ç‚ºä»€éº¼é‚„è¦ä¿ç•™ `lora_training_template.toml`ï¼Ÿ

**A:** å› ç‚ºå®ƒæ˜¯å¾ˆå¥½çš„**åƒè€ƒæ–‡æª”**ï¼Œè¨˜éŒ„äº†æ‰€æœ‰åƒæ•¸åŠå…¶èªªæ˜Žã€‚ä½ å¯ä»¥æ ¹æ“šå®ƒå‰µå»ºè¨“ç·´è…³æœ¬æˆ– Python é…ç½®ã€‚

### Q: æˆ‘èƒ½åªç”¨ `dataset_config_template.toml` å—Žï¼Ÿ

**A:** ä¸èƒ½ã€‚æ•¸æ“šé›†é…ç½®åªåŒ…å«æ•¸æ“šé›†éƒ¨åˆ†ï¼Œä½ é‚„éœ€è¦é€šéŽ CLI æˆ–è…³æœ¬å‚³éžè¨“ç·´åƒæ•¸ã€‚

### Q: æŽ¨è–¦ä½¿ç”¨å“ªç¨®æ–¹å¼ï¼Ÿ

**A:** æŽ¨è–¦ä½¿ç”¨ **æ•¸æ“šé›† TOML + è¨“ç·´è…³æœ¬** çš„çµ„åˆæ–¹å¼ï¼š
- æ•¸æ“šé›†é…ç½®ç”¨ `dataset.toml`
- è¨“ç·´åƒæ•¸å¯«åœ¨ `train.sh` æˆ– `train.py` è…³æœ¬ä¸­

---

**æœ€å¾Œæ›´æ–°ï¼š** 2025-11-11
